{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26073d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import os\n",
    "import sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import ShuffleSplit, cross_val_score, train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.fft import rfft, irfft\n",
    "from torch.nn.functional import normalize\n",
    "from scipy.signal import welch\n",
    "from scipy import signal\n",
    "import math\n",
    "\n",
    "NUM_CHANNELS = 1\n",
    "SEQUENCE_LENGTH = 9760\n",
    "LAMBDA_GP = 5\n",
    "\n",
    "def compute_psd(data, fs, nperseg=256, noverlap=None):\n",
    "    \"\"\"\n",
    "    Compute Power Spectral Density (PSD) using the Welch method.\n",
    "\n",
    "    Parameters:\n",
    "        data (array): EEG data array with shape (n_channels, n_samples).\n",
    "        fs (float): Sampling frequency.\n",
    "        nperseg (int): Length of each segment for PSD estimation.\n",
    "        noverlap (int): Number of overlapping samples between segments.\n",
    "    \n",
    "    Returns:\n",
    "        freqs (array): Frequency values.\n",
    "        psd (array): Power Spectral Density values.\n",
    "    \"\"\"\n",
    "    n_channels, n_samples = data.shape\n",
    "    psd = np.zeros((n_channels, nperseg // 2 + 1))\n",
    "\n",
    "    for ch_idx in range(n_channels):\n",
    "        f, Pxx = plt.psd(data[ch_idx], Fs=fs, NFFT=256, noverlap=128, window=np.hanning(256), scale_by_freq=True)\n",
    "        # Add a small epsilon to avoid zero values\n",
    "        psd[ch_idx] = Pxx + 1e-10\n",
    "\n",
    "    return f, psd\n",
    "\n",
    "def average_across_arrays(generated_data):\n",
    "    return generated_data.mean(dim=0)\n",
    "\n",
    "# Set the sampling frequency\n",
    "fs = 160.0\n",
    "\n",
    "def weights_init(model):\n",
    "    for m in model.modules():\n",
    "      if isinstance(m, (nn.Conv1d)):\n",
    "        nn.init.xavier_uniform_(m.weight.data, math.sqrt(2.0))\n",
    "\n",
    "def gradient_penalty(D, real, fake):\n",
    "    alpha = torch.rand((BATCH_SIZE, 1, 1)).repeat(1, NUM_CHANNELS, SEQUENCE_LENGTH)\n",
    "    interpolated_seq = real * alpha + fake * (1 - alpha)\n",
    "\n",
    "    # Calculate discriminator scores\n",
    "    mixed_scores = D(interpolated_seq)\n",
    "\n",
    "    # Take the gradient of the scores with respect to the images\n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs=interpolated_seq,\n",
    "        outputs=mixed_scores,\n",
    "        grad_outputs=torch.ones_like(mixed_scores),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "    gradient = gradient.view(gradient.shape[0], -1)\n",
    "    gradient_norm = gradient.norm(2, dim=1)\n",
    "    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n",
    "    return gradient_penalty\n",
    "\n",
    "def plot_everything(generated_data, gen_err, critic_err):\n",
    "    generated_data = generated_data.detach()\n",
    "    \n",
    "    # plotting generated data\n",
    "    values = generated_data[0, 0, :]\n",
    "    plt.plot(values.tolist())\n",
    "    plt.show()\n",
    "\n",
    "    # plotting PSD\n",
    "    averaged_data = average_across_arrays(generated_data)\n",
    "    freqs, psd = compute_psd(averaged_data, fs)\n",
    "    plt.figure(figsize=(10, 6))  # Add this line to create a single figure\n",
    "    for ch_idx in range(NUM_CHANNELS):\n",
    "        plt.semilogy(freqs, psd[ch_idx], label=f'Channel {ch_idx + 1}')\n",
    "    plt.xlabel('Frequency (Hz)')\n",
    "    plt.ylabel('Power/Frequency (dB/Hz)')\n",
    "    plt.show()\n",
    "\n",
    "    # plotting G vs D losses\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "    plt.plot(gen_err,label=\"Generator\")\n",
    "    plt.plot(critic_err,label=\"Critic\")\n",
    "    plt.xlabel(\"iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 83 batch size for eyes_open\n",
    "# 76 batch size for eyes_closed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84a255b",
   "metadata": {},
   "source": [
    "# resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fef85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 300\n",
    "PRINT_INTERVAL = 10\n",
    "BATCH_SIZE = 76 \n",
    "train_loader = DataLoader(\n",
    "    TensorDataset(torch.tensor(np.load(\"training-closed-ch21.npy\")).detach()), \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True\n",
    ")\n",
    "torch.autograd.set_detect_anomaly(False)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.res1_acti1 = nn.Sequential(nn.BatchNorm1d(200), nn.ReLU(inplace=False))\n",
    "        self.res1_conv1 = nn.Conv1d(200, 200, kernel_size=9, stride=1, padding=4)\n",
    "        self.res1_acti2 = nn.Sequential(nn.BatchNorm1d(200), nn.ReLU(inplace=False))\n",
    "        self.res1_conv2 = nn.Conv1d(200, 200, kernel_size=9, stride=1, padding=4)\n",
    "\n",
    "        self.res2_acti1 = nn.Sequential(nn.BatchNorm1d(200), nn.ReLU(inplace=False))\n",
    "        self.res2_conv1 = nn.Conv1d(200, 200, kernel_size=9, stride=1, padding=4)\n",
    "        self.res2_acti2 = nn.Sequential(nn.BatchNorm1d(200), nn.ReLU(inplace=False))\n",
    "        self.res2_conv2 = nn.Conv1d(200, 200, kernel_size=9, stride=1, padding=4)\n",
    "\n",
    "        self.res3_acti1 = nn.Sequential(nn.BatchNorm1d(200), nn.ReLU(inplace=False))\n",
    "        self.res3_conv1 = nn.Conv1d(200, 200, kernel_size=9, stride=1, padding=4)\n",
    "        self.res3_acti2 = nn.Sequential(nn.BatchNorm1d(200), nn.ReLU(inplace=False))\n",
    "        self.res3_conv2 = nn.Conv1d(200, 200, kernel_size=9, stride=1, padding=4)\n",
    "\n",
    "        self.res4_acti1 = nn.Sequential(nn.BatchNorm1d(200), nn.ReLU(inplace=False))\n",
    "        self.res4_conv1 = nn.Conv1d(200, 200, kernel_size=9, stride=1, padding=4)\n",
    "        self.res4_acti2 = nn.Sequential(nn.BatchNorm1d(200), nn.ReLU(inplace=False))\n",
    "        self.res4_conv2 = nn.Conv1d(200, 200, kernel_size=9, stride=1, padding=4)\n",
    "\n",
    "        self.res5_acti1 = nn.Sequential(nn.BatchNorm1d(200), nn.ReLU(inplace=False))\n",
    "        self.res5_conv1 = nn.Conv1d(200, 200, kernel_size=9, stride=1, padding=4)\n",
    "        self.res5_acti2 = nn.Sequential(nn.BatchNorm1d(200), nn.ReLU(inplace=False))\n",
    "        self.res5_conv2 = nn.Conv1d(200, 1, kernel_size=9, stride=1, padding=4)\n",
    "\n",
    "        self.batch = nn.BatchNorm1d(1)\n",
    "        self.ReLU = nn.ReLU(True)\n",
    "        self.identity_transform = nn.Conv1d(200, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):        \n",
    "        identity = x\n",
    "        x = self.res1_acti1(x)\n",
    "        x = self.res1_conv1(F.interpolate(x, scale_factor=2, mode='linear', align_corners=False))\n",
    "        x = self.res1_acti2(x)\n",
    "        x = self.res1_conv2(x)\n",
    "        identity = F.interpolate(identity, scale_factor=2, mode='linear', align_corners=False)\n",
    "        x = torch.add(x, identity)\n",
    "        \n",
    "        identity = x\n",
    "        x = self.res2_acti1(x)\n",
    "        x = self.res2_conv1(F.interpolate(x, scale_factor=2, mode='linear', align_corners=False))\n",
    "        x = self.res2_acti2(x)\n",
    "        x = self.res2_conv2(x)\n",
    "        identity = F.interpolate(identity, scale_factor=2, mode='linear', align_corners=False)\n",
    "        x = torch.add(x, identity)\n",
    "        \n",
    "        identity = x\n",
    "        x = self.res3_acti1(x)\n",
    "        x = self.res3_conv1(F.interpolate(x, scale_factor=2, mode='linear', align_corners=False))\n",
    "        x = self.res3_acti2(x)\n",
    "        x = self.res3_conv2(x)\n",
    "        identity = F.interpolate(identity, scale_factor=2, mode='linear', align_corners=False)\n",
    "        x = torch.add(x, identity)\n",
    "        \n",
    "        identity = x\n",
    "        x = self.res4_acti1(x)\n",
    "        x = self.res4_conv1(F.interpolate(x, scale_factor=2, mode='linear', align_corners=False))\n",
    "        x = self.res4_acti2(x)\n",
    "        x = self.res4_conv2(x)\n",
    "        identity = F.interpolate(identity, scale_factor=2, mode='linear', align_corners=False)\n",
    "        x = torch.add(x, identity)\n",
    "        \n",
    "        identity = x\n",
    "        x = self.res5_acti1(x)\n",
    "        x = self.res5_conv1(F.interpolate(x, scale_factor=2, mode='linear', align_corners=False))\n",
    "        x = self.res5_acti2(x)\n",
    "        x = self.res5_conv2(x)\n",
    "        identity = F.interpolate(identity, scale_factor=2, mode='linear', align_corners=False)\n",
    "        identity = self.identity_transform(identity)\n",
    "        x = torch.add(x, identity)\n",
    "        \n",
    "        x = self.batch(x)\n",
    "        x = self.ReLU(x)\n",
    "        return x\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.res1_acti1 = nn.ReLU(inplace=False)\n",
    "        self.res1_conv1 = nn.Conv1d(1, 200, kernel_size=9, stride=1, padding=4)\n",
    "        self.res1_acti2 = nn.ReLU(inplace=False)\n",
    "        self.res1_conv2 = nn.Conv1d(200, 200, kernel_size=9, stride=1, padding=4)\n",
    "\n",
    "        self.res2_acti1 = nn.ReLU(inplace=False)\n",
    "        self.res2_conv1 = nn.Conv1d(200, 200, kernel_size=9, stride=1, padding=4)\n",
    "        self.res2_acti2 = nn.ReLU(inplace=False)\n",
    "        self.res2_conv2 = nn.Conv1d(200, 200, kernel_size=9, stride=1, padding=4)\n",
    "\n",
    "        self.res3_acti1 = nn.ReLU(inplace=False)\n",
    "        self.res3_conv1 = nn.Conv1d(200, 200, kernel_size=9, stride=1, padding=4)\n",
    "        self.res3_acti2 = nn.ReLU(inplace=False)\n",
    "        self.res3_conv2 = nn.Conv1d(200, 200, kernel_size=9, stride=1, padding=4)\n",
    "\n",
    "        self.res4_acti1 = nn.ReLU(inplace=False)\n",
    "        self.res4_conv1 = nn.Conv1d(200, 200, kernel_size=9, stride=1, padding=4)\n",
    "        self.res4_acti2 = nn.ReLU(inplace=False)\n",
    "        self.res4_conv2 = nn.Conv1d(200, 200, kernel_size=9, stride=1, padding=4)\n",
    "\n",
    "        self.res5_acti1 = nn.ReLU(inplace=False)\n",
    "        self.res5_conv1 = nn.Conv1d(200, 200, kernel_size=9, stride=1, padding=4)\n",
    "        self.res5_acti2 = nn.ReLU(inplace=False)\n",
    "        self.res5_conv2 = nn.Conv1d(200, 200, kernel_size=9, stride=1, padding=4)\n",
    "\n",
    "        self.res6_acti1 = nn.ReLU(inplace=False)\n",
    "        self.res6_conv1 = nn.Conv1d(200, 200, kernel_size=9, stride=1, padding=4)\n",
    "        self.res6_acti2 = nn.ReLU(inplace=False)\n",
    "        self.res6_conv2 = nn.Conv1d(200, 1, kernel_size=9, stride=1, padding=4)\n",
    "\n",
    "        self.ReLU = nn.ReLU(inplace=False)\n",
    "        self.end = nn.Linear(152, 1)\n",
    "        self.identity_transform = nn.Conv1d(200, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        identity = x\n",
    "        x = self.res1_acti1(x)\n",
    "        x = self.res1_conv1(x)\n",
    "        x = self.res1_acti2(x)\n",
    "        x = self.res1_conv2(x)\n",
    "        x = F.avg_pool1d(x, 2)\n",
    "        identity = F.avg_pool1d(identity, 2)\n",
    "        x = torch.add(x, identity)\n",
    "        \n",
    "        identity = x\n",
    "        x = self.res2_acti1(x)\n",
    "        x = self.res2_conv1(x)\n",
    "        x = self.res2_acti2(x)\n",
    "        x = self.res2_conv2(x)\n",
    "        x = F.avg_pool1d(x, 2)\n",
    "        identity = F.avg_pool1d(identity, 2)\n",
    "        x = torch.add(x, identity)\n",
    "        \n",
    "        identity = x\n",
    "        x = self.res3_acti1(x)\n",
    "        x = self.res3_conv1(x)\n",
    "        x = self.res3_acti2(x)\n",
    "        x = self.res3_conv2(x)\n",
    "        x = F.avg_pool1d(x, 2)\n",
    "        identity = F.avg_pool1d(identity, 2)\n",
    "        x = torch.add(x, identity)\n",
    "        \n",
    "        identity = x\n",
    "        x = self.res4_acti1(x)\n",
    "        x = self.res4_conv1(x)\n",
    "        x = self.res4_acti2(x)\n",
    "        x = self.res4_conv2(x)\n",
    "        x = F.avg_pool1d(x, 2)\n",
    "        identity = F.avg_pool1d(identity, 2)\n",
    "        x = torch.add(x, identity)\n",
    "        \n",
    "        identity = x\n",
    "        x = self.res5_acti1(x)\n",
    "        x = self.res5_conv1(x)\n",
    "        x = self.res5_acti2(x)\n",
    "        x = self.res5_conv2(x)\n",
    "        x = F.avg_pool1d(x, 2)\n",
    "        identity = F.avg_pool1d(identity, 2)\n",
    "        x = torch.add(x, identity)\n",
    "        \n",
    "        identity = x\n",
    "        x = self.res6_acti1(x)\n",
    "        x = self.res6_conv1(x)\n",
    "        x = self.res6_acti2(x)\n",
    "        x = self.res6_conv2(x)\n",
    "        x = F.avg_pool1d(x, 2)\n",
    "        identity = F.avg_pool1d(identity, 2)\n",
    "        identity = self.identity_transform(identity)\n",
    "        x = torch.add(x, identity)\n",
    "        \n",
    "        x = self.ReLU(x)\n",
    "        x = self.end(x)\n",
    "        return x\n",
    "    \n",
    "def train():\n",
    "    critic_err, gen_err = [], []\n",
    "    G, D = Generator(), Discriminator()\n",
    "    weights_init(G)\n",
    "    weights_init(D)\n",
    "    critic_optimizer = optim.RMSprop(D.parameters(), lr=1e-5)\n",
    "    gen_optimizer = optim.RMSprop(G.parameters(), lr=1e-5)\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        (real, ) = next(iter(train_loader))\n",
    "        real.requires_grad = True\n",
    "\n",
    "        for _ in range(5):\n",
    "            fake = G(torch.randn(BATCH_SIZE, 200, 305))\n",
    "            critic_real = D(real).reshape(-1)\n",
    "            critic_fake = D(fake).reshape(-1)\n",
    "            D.zero_grad()            \n",
    "            loss_critic = -torch.mean(critic_real) + torch.mean(critic_fake) + (gradient_penalty(D, real, fake) * LAMBDA_GP)\n",
    "            loss_critic.backward()\n",
    "            critic_optimizer.step()  \n",
    "\n",
    "        fake = G(torch.randn(BATCH_SIZE, 200, 305))\n",
    "        gen_fake = D(fake).reshape(-1)\n",
    "        G.zero_grad()\n",
    "        loss_gen = -torch.mean(gen_fake)\n",
    "        loss_gen.backward()\n",
    "        gen_optimizer.step()\n",
    "        \n",
    "        critic_err.append(loss_critic.item())\n",
    "        gen_err.append(loss_gen.item())\n",
    "\n",
    "        if epoch % PRINT_INTERVAL == 0:\n",
    "            print(\"Epoch %s: Critic error (%s) Generator err (%s)\" % (epoch, critic_err, gen_err))\n",
    "            plot_everything(fake, critic_err, gen_err)\n",
    "\n",
    "    return critic_err, gen_err, fake, D, G\n",
    "\n",
    "critic_err, gen_err, generated, disc, gen = train()\n",
    "\n",
    "torch.save(gen, 'generator-v14.pt')\n",
    "torch.save(disc, 'discriminator-v14.pt')\n",
    "np.save('closed-v14.npy', generated.detach())\n",
    "plot_everything(generated, gen_err, critic_err)\n",
    "generated_data_closed = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8abb92f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
