{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfvoSY4W72Kv",
        "outputId": "66798e55-5d31-4468-efc4-db87d2ca11e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "hello GPU\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from abc import ABC, abstractmethod\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import seaborn as sns\n",
        "from scipy.linalg import sqrtm\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  print(\"hello GPU\")\n",
        "else:\n",
        "  print(\"sadge\")\n",
        "\n",
        "\n",
        "class BaseModel(nn.Module, ABC):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    @abstractmethod\n",
        "    def forward(self, x):\n",
        "        pass\n",
        "\n",
        "    @property\n",
        "    def device(self):\n",
        "        return next(self.parameters()).device\n",
        "\n",
        "    def restore_checkpoint(self, ckpt_file, optimizer=None):\n",
        "        if not ckpt_file:\n",
        "            raise ValueError(\"No checkpoint file to be restored.\")\n",
        "\n",
        "        try:\n",
        "            ckpt_dict = torch.load(ckpt_file)\n",
        "        except RuntimeError:\n",
        "            ckpt_dict = torch.load(ckpt_file,\n",
        "                                   map_location=lambda storage, loc: storage)\n",
        "\n",
        "        # Restore model weights\n",
        "        self.load_state_dict(ckpt_dict['model_state_dict'])\n",
        "\n",
        "        # Restore optimizer status if existing. Evaluation doesn't need this\n",
        "        if optimizer:\n",
        "            optimizer.load_state_dict(ckpt_dict['optimizer_state_dict'])\n",
        "\n",
        "        # Return global step\n",
        "        return ckpt_dict['global_step']\n",
        "\n",
        "    def save_checkpoint(self,\n",
        "                        directory,\n",
        "                        global_step,\n",
        "                        optimizer=None,\n",
        "                        name=None):\n",
        "        # Create directory to save to\n",
        "        if not os.path.exists(directory):\n",
        "            os.makedirs(directory)\n",
        "\n",
        "        # Build checkpoint dict to save.\n",
        "        ckpt_dict = {\n",
        "            'model_state_dict': self.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict() if optimizer is not None else None,\n",
        "            'global_step': global_step\n",
        "        }\n",
        "\n",
        "        # Save the file with specific name\n",
        "        if name is None:\n",
        "            name = \"{}_{}_steps.pth\".format(\n",
        "                os.path.basename(directory),  # netD or netG\n",
        "                global_step)\n",
        "\n",
        "        torch.save(ckpt_dict, os.path.join(directory, name))\n",
        "\n",
        "    def count_params(self):\n",
        "        num_total_params = sum(p.numel() for p in self.parameters())\n",
        "        num_trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "        return num_total_params, num_trainable_params\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Discriminator(BaseModel):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__()\n",
        "        self.count = 0\n",
        "        self.errD_array = []\n",
        "        self.bce = nn.BCELoss()\n",
        "        self.ndf = 100\n",
        "\n",
        "        self.sblock1 = nn.Linear(3152,2700)\n",
        "        self.relu = nn.ReLU(True)\n",
        "        self.sblock2 = nn.Linear(2700,2000)\n",
        "        self.sblock3 = nn.Linear(2000,1500)\n",
        "        self.sblock4 = nn.Linear(1500,1100)\n",
        "        self.sblock5 = nn.Linear(1100,512)\n",
        "        self.sblock8 = nn.Linear(512,128)\n",
        "        self.sblock9 = nn.Linear(128,64)\n",
        "        self.sblock10 = nn.Linear(64,1)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "        # Final classification layer\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.dropout(self.relu(self.sblock1(x)))\n",
        "        h = self.dropout(self.relu(self.sblock2(h)))\n",
        "        h = self.dropout(self.relu(self.sblock3(h)))\n",
        "        h = self.dropout(self.relu(self.sblock4(h)))\n",
        "        h = self.dropout(self.relu(self.sblock5(h)))\n",
        "        h = self.dropout(self.relu(self.sblock8(h)))\n",
        "        h = self.relu(self.sblock9(h))\n",
        "        h = self.sblock10(h)\n",
        "        h = self.sigmoid(h)\n",
        "        return h.view(h.shape[0], 64)\n",
        "\n",
        "    def compute_loss(self, output, actual):\n",
        "        return self.bce(output, actual)\n",
        "\n",
        "def train(open, closed, dupe=False):\n",
        "  device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
        "  netD = Discriminator().to(device)\n",
        "  optimizer = torch.optim.Adam(netD.parameters(), 0.0001, (0.5, 0.99))\n",
        "\n",
        "  train_lossD = []\n",
        "  train_accuracyD = []\n",
        "  test_lossD = []\n",
        "  test_accuracyD = []\n",
        "\n",
        "  # Split data into training and testing sets\n",
        "  train_open, test_open = train_test_split(open, test_size=0.2, random_state=42)\n",
        "  train_closed, test_closed = train_test_split(closed, test_size=0.2, random_state=42)\n",
        "\n",
        "  if (dupe):\n",
        "      train_open = np.concatenate((train_open, train_open))\n",
        "      train_closed = np.concatenate((train_closed, train_closed))\n",
        "\n",
        "  # Move data to tensors and to device\n",
        "  train_open = torch.tensor(train_open).float().to(device)\n",
        "  train_closed = torch.tensor(train_closed).float().to(device)\n",
        "  test_open = torch.tensor(test_open).float().to(device)\n",
        "  test_closed = torch.tensor(test_closed).float().to(device)\n",
        "\n",
        "  for epoch in range(20):\n",
        "    # Training\n",
        "    netD.zero_grad()\n",
        "\n",
        "    train_output_open = netD.forward(train_open)\n",
        "    train_output_closed = netD.forward(train_closed)\n",
        "\n",
        "    train_vals_open = torch.full(train_output_open.shape, 1.0, dtype=torch.float, device=device)\n",
        "    train_vals_closed = torch.full(train_output_closed.shape, 0.0, dtype=torch.float, device=device)\n",
        "\n",
        "    train_pred_labels = torch.cat((train_output_open, train_output_closed))\n",
        "    train_actual_labels = torch.cat((train_vals_open, train_vals_closed))\n",
        "\n",
        "    train_accuracy = ((train_pred_labels > 0.5) == train_actual_labels).float().mean()\n",
        "    train_accuracyD.append(train_accuracy.item())\n",
        "\n",
        "    # Compute training loss\n",
        "    train_errD_real = netD.compute_loss(train_output_open, train_vals_open)\n",
        "    train_errD_fake = netD.compute_loss(train_output_closed, train_vals_closed)\n",
        "    train_errD = train_errD_real + train_errD_fake\n",
        "    train_errD.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    train_lossD.append(train_errD.item())\n",
        "\n",
        "    # Testing\n",
        "    with torch.no_grad():\n",
        "      test_output_open = netD.forward(test_open)\n",
        "      test_output_closed = netD.forward(test_closed)\n",
        "\n",
        "      # Compute classification accuracy\n",
        "      test_vals_open = torch.full(test_output_open.shape, 1.0, dtype=torch.float, device=device)\n",
        "      test_vals_closed = torch.full(test_output_closed.shape, 0.0, dtype=torch.float, device=device)\n",
        "\n",
        "      test_pred_labels = torch.cat((test_output_open, test_output_closed))\n",
        "      test_actual_labels = torch.cat((test_vals_open, test_vals_closed))\n",
        "\n",
        "      test_accuracy = ((test_pred_labels > 0.5) == test_actual_labels).float().mean()\n",
        "\n",
        "      # Bin guess into T/F for confusion matrix\n",
        "      test_output_open_cm = (test_output_open.mean(dim=1, keepdim=True) > 0.5).float()\n",
        "      test_output_closed_cm = (test_output_closed.mean(dim=1, keepdim=True) > 0.5).float()\n",
        "      test_vals_open_cm = (test_vals_open.mean(dim=1, keepdim=True) > 0.5).float()\n",
        "      test_vals_closed_cm = (test_vals_closed.mean(dim=1, keepdim=True) > 0.5).float()\n",
        "\n",
        "      # Compute testing loss\n",
        "      test_errD_real = netD.compute_loss(test_output_open, test_vals_open)\n",
        "      test_errD_fake = netD.compute_loss(test_output_closed, test_vals_closed)\n",
        "      test_errD = test_errD_real + test_errD_fake\n",
        "\n",
        "      test_accuracyD.append(test_accuracy.item())\n",
        "      test_lossD.append(test_errD.item())\n",
        "\n",
        "    # if epoch % 20 == 0:\n",
        "    #   print('Epoch', epoch)\n",
        "    #   print('Train Loss:', train_errD.item(), 'Test Loss:', test_errD.item())\n",
        "    #   print('Train Accuracy:', train_accuracy.item(), 'Test Accuracy:', test_accuracy.item())\n",
        "    #   plt.figure(figsize=(10,4))\n",
        "    #   plt.subplot(1, 2, 1)\n",
        "    #   plt.plot(train_lossD, label='Training Loss')\n",
        "    #   plt.plot(test_lossD, label='Testing Loss')\n",
        "    #   plt.legend()\n",
        "    #   plt.subplot(1, 2, 2)\n",
        "    #   plt.plot(train_accuracyD, label='Training Accuracy')\n",
        "    #   plt.plot(test_accuracyD, label='Testing Accuracy')\n",
        "    #   plt.legend()\n",
        "    #   plt.show()\n",
        "\n",
        "  print('Final Test Accuracy:', test_accuracy.item())\n",
        "  ##return torch.cat((test_output_open_cm, test_output_closed_cm)), torch.cat((test_vals_open_cm, test_vals_closed_cm)), netD\n",
        "  return test_accuracy.item()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "open = np.load(\"/content/gdrive/My Drive/Research_Paper/Training/normalized-training-open-64ch.npy\")\n",
        "closed = np.load(\"/content/gdrive/My Drive/Research_Paper/Training/normalized-training-closed-64ch.npy\")\n",
        "\n",
        "test_100_accuracies = []\n",
        "\n",
        "for i in range(100):\n",
        "  accuracy = train(open, closed)\n",
        "  test_100_accuracies.append(accuracy)\n",
        "\n",
        "print(\"Avg test accuracy: \", np.mean(test_100_accuracies))\n",
        "\n"
      ],
      "metadata": {
        "id": "rHkTNp3bKUgE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "901cb74a-37e9-484a-9479-8935b42ce3a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Test Accuracy: 0.6510416865348816\n",
            "Final Test Accuracy: 0.6579861044883728\n",
            "Final Test Accuracy: 0.6571180820465088\n",
            "Final Test Accuracy: 0.65625\n",
            "Final Test Accuracy: 0.6215277910232544\n",
            "Final Test Accuracy: 0.6684027910232544\n",
            "Final Test Accuracy: 0.6605902910232544\n",
            "Final Test Accuracy: 0.6380208134651184\n",
            "Final Test Accuracy: 0.6267361044883728\n",
            "Final Test Accuracy: 0.6449652910232544\n",
            "Final Test Accuracy: 0.6354166865348816\n",
            "Final Test Accuracy: 0.6640625\n",
            "Final Test Accuracy: 0.6484375\n",
            "Final Test Accuracy: 0.6362847089767456\n",
            "Final Test Accuracy: 0.65625\n",
            "Final Test Accuracy: 0.6701388955116272\n",
            "Final Test Accuracy: 0.6215277910232544\n",
            "Final Test Accuracy: 0.663194477558136\n",
            "Final Test Accuracy: 0.647569477558136\n",
            "Final Test Accuracy: 0.6501736044883728\n",
            "Final Test Accuracy: 0.6605902910232544\n",
            "Final Test Accuracy: 0.6284722089767456\n",
            "Final Test Accuracy: 0.6414930820465088\n",
            "Final Test Accuracy: 0.6579861044883728\n",
            "Final Test Accuracy: 0.6449652910232544\n",
            "Final Test Accuracy: 0.6449652910232544\n",
            "Final Test Accuracy: 0.6440972089767456\n",
            "Final Test Accuracy: 0.5998263955116272\n",
            "Final Test Accuracy: 0.6571180820465088\n",
            "Final Test Accuracy: 0.6510416865348816\n",
            "Final Test Accuracy: 0.6510416865348816\n",
            "Final Test Accuracy: 0.6371527910232544\n",
            "Final Test Accuracy: 0.647569477558136\n",
            "Final Test Accuracy: 0.6223958134651184\n",
            "Final Test Accuracy: 0.6388888955116272\n",
            "Final Test Accuracy: 0.6258680820465088\n",
            "Final Test Accuracy: 0.6145833134651184\n",
            "Final Test Accuracy: 0.624131977558136\n",
            "Final Test Accuracy: 0.6432291865348816\n",
            "Final Test Accuracy: 0.655381977558136\n",
            "Final Test Accuracy: 0.6493055820465088\n",
            "Final Test Accuracy: 0.6293402910232544\n",
            "Final Test Accuracy: 0.5703125\n",
            "Final Test Accuracy: 0.5390625\n",
            "Final Test Accuracy: 0.6493055820465088\n",
            "Final Test Accuracy: 0.6467013955116272\n",
            "Final Test Accuracy: 0.655381977558136\n",
            "Final Test Accuracy: 0.6493055820465088\n",
            "Final Test Accuracy: 0.6336805820465088\n",
            "Final Test Accuracy: 0.6328125\n",
            "Final Test Accuracy: 0.6458333134651184\n",
            "Final Test Accuracy: 0.6701388955116272\n",
            "Final Test Accuracy: 0.640625\n",
            "Final Test Accuracy: 0.6579861044883728\n",
            "Final Test Accuracy: 0.6232638955116272\n",
            "Final Test Accuracy: 0.609375\n",
            "Final Test Accuracy: 0.6345486044883728\n",
            "Final Test Accuracy: 0.6284722089767456\n",
            "Final Test Accuracy: 0.5711805820465088\n",
            "Final Test Accuracy: 0.6588541865348816\n",
            "Final Test Accuracy: 0.6684027910232544\n",
            "Final Test Accuracy: 0.6588541865348816\n",
            "Final Test Accuracy: 0.6501736044883728\n",
            "Final Test Accuracy: 0.6432291865348816\n",
            "Final Test Accuracy: 0.65625\n",
            "Final Test Accuracy: 0.6657986044883728\n",
            "Final Test Accuracy: 0.6258680820465088\n",
            "Final Test Accuracy: 0.65625\n",
            "Final Test Accuracy: 0.6032986044883728\n",
            "Final Test Accuracy: 0.639756977558136\n",
            "Final Test Accuracy: 0.6493055820465088\n",
            "Final Test Accuracy: 0.6692708134651184\n",
            "Final Test Accuracy: 0.6597222089767456\n",
            "Final Test Accuracy: 0.6336805820465088\n",
            "Final Test Accuracy: 0.6432291865348816\n",
            "Final Test Accuracy: 0.6467013955116272\n",
            "Final Test Accuracy: 0.6501736044883728\n",
            "Final Test Accuracy: 0.5885416865348816\n",
            "Final Test Accuracy: 0.6440972089767456\n",
            "Final Test Accuracy: 0.6545138955116272\n",
            "Final Test Accuracy: 0.6414930820465088\n",
            "Final Test Accuracy: 0.592881977558136\n",
            "Final Test Accuracy: 0.6336805820465088\n",
            "Final Test Accuracy: 0.6423611044883728\n",
            "Final Test Accuracy: 0.6458333134651184\n",
            "Final Test Accuracy: 0.6614583134651184\n",
            "Final Test Accuracy: 0.6519097089767456\n",
            "Final Test Accuracy: 0.6232638955116272\n",
            "Final Test Accuracy: 0.640625\n",
            "Final Test Accuracy: 0.6458333134651184\n",
            "Final Test Accuracy: 0.6440972089767456\n",
            "Final Test Accuracy: 0.6302083134651184\n",
            "Final Test Accuracy: 0.6362847089767456\n",
            "Final Test Accuracy: 0.6432291865348816\n",
            "Final Test Accuracy: 0.639756977558136\n",
            "Final Test Accuracy: 0.6458333134651184\n",
            "Final Test Accuracy: 0.6345486044883728\n",
            "Final Test Accuracy: 0.6536458134651184\n",
            "Final Test Accuracy: 0.6293402910232544\n",
            "Final Test Accuracy: 0.631944477558136\n",
            "Avg test accuracy:  0.6403732711076736\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "open = np.load(\"/content/gdrive/My Drive/Research_Paper/Training/normalized-training-open-64ch.npy\")\n",
        "closed = np.load(\"/content/gdrive/My Drive/Research_Paper/Training/normalized-training-closed-64ch.npy\")\n",
        "open_generated1 = np.load(\"/content/gdrive/My Drive/Research_Paper/generated-data/generated-open-1.npy\")\n",
        "closed_generated1 = np.load(\"/content/gdrive/My Drive/Research_Paper/generated-data/generated-closed-1.npy\")\n",
        "open_generated2 = np.load(\"/content/gdrive/My Drive/Research_Paper/generated-data/generated-open-2.npy\")\n",
        "closed_generated2 = np.load(\"/content/gdrive/My Drive/Research_Paper/generated-data/generated-closed-2.npy\")\n",
        "\n",
        "open = np.concatenate((open, open_generated1))\n",
        "closed = np.concatenate((closed, closed_generated1))\n",
        "open = np.concatenate((open, open_generated2))\n",
        "closed = np.concatenate((closed, closed_generated2))\n",
        "test_100_accuracies = []\n",
        "\n",
        "for i in range(100):\n",
        "  accuracy = train(open, closed)\n",
        "  test_100_accuracies.append(accuracy)\n",
        "\n",
        "print(\"Avg test accuracy: \", np.mean(test_100_accuracies))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fo6_hqRtYvto",
        "outputId": "4d56ff3e-4885-486f-e555-7e7437f584ea"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Test Accuracy: 0.8307783007621765\n",
            "Final Test Accuracy: 0.8307783007621765\n",
            "Final Test Accuracy: 0.8605542778968811\n",
            "Final Test Accuracy: 0.8346108794212341\n",
            "Final Test Accuracy: 0.8337264060974121\n",
            "Final Test Accuracy: 0.8354952931404114\n",
            "Final Test Accuracy: 0.8555424809455872\n",
            "Final Test Accuracy: 0.8428655862808228\n",
            "Final Test Accuracy: 0.829009473323822\n",
            "Final Test Accuracy: 0.8307783007621765\n",
            "Final Test Accuracy: 0.8337264060974121\n",
            "Final Test Accuracy: 0.8325471878051758\n",
            "Final Test Accuracy: 0.838738203048706\n",
            "Final Test Accuracy: 0.8534787893295288\n",
            "Final Test Accuracy: 0.8266509771347046\n",
            "Final Test Accuracy: 0.8419811725616455\n",
            "Final Test Accuracy: 0.8584905862808228\n",
            "Final Test Accuracy: 0.8357900977134705\n",
            "Final Test Accuracy: 0.8275353908538818\n",
            "Final Test Accuracy: 0.8293042778968811\n",
            "Final Test Accuracy: 0.839327871799469\n",
            "Final Test Accuracy: 0.8295990824699402\n",
            "Final Test Accuracy: 0.8354952931404114\n",
            "Final Test Accuracy: 0.8416863083839417\n",
            "Final Test Accuracy: 0.8605542778968811\n",
            "Final Test Accuracy: 0.8307783007621765\n",
            "Final Test Accuracy: 0.8452240824699402\n",
            "Final Test Accuracy: 0.8405070900917053\n",
            "Final Test Accuracy: 0.8461084961891174\n",
            "Final Test Accuracy: 0.8434551954269409\n",
            "Final Test Accuracy: 0.8349056839942932\n",
            "Final Test Accuracy: 0.8354952931404114\n",
            "Final Test Accuracy: 0.8605542778968811\n",
            "Final Test Accuracy: 0.8316627740859985\n",
            "Final Test Accuracy: 0.8328419923782349\n",
            "Final Test Accuracy: 0.8360849022865295\n",
            "Final Test Accuracy: 0.8363797068595886\n",
            "Final Test Accuracy: 0.8378537893295288\n",
            "Final Test Accuracy: 0.8278301954269409\n",
            "Final Test Accuracy: 0.834316074848175\n",
            "Final Test Accuracy: 0.833136796951294\n",
            "Final Test Accuracy: 0.8316627740859985\n",
            "Final Test Accuracy: 0.8304834961891174\n",
            "Final Test Accuracy: 0.8723466992378235\n",
            "Final Test Accuracy: 0.8522995114326477\n",
            "Final Test Accuracy: 0.8449292778968811\n",
            "Final Test Accuracy: 0.8337264060974121\n",
            "Final Test Accuracy: 0.8340212106704712\n",
            "Final Test Accuracy: 0.8328419923782349\n",
            "Final Test Accuracy: 0.8328419923782349\n",
            "Final Test Accuracy: 0.8434551954269409\n",
            "Final Test Accuracy: 0.8410966992378235\n",
            "Final Test Accuracy: 0.8301886916160583\n",
            "Final Test Accuracy: 0.8531839847564697\n",
            "Final Test Accuracy: 0.8316627740859985\n",
            "Final Test Accuracy: 0.833431601524353\n",
            "Final Test Accuracy: 0.823113203048706\n",
            "Final Test Accuracy: 0.8301886916160583\n",
            "Final Test Accuracy: 0.8316627740859985\n",
            "Final Test Accuracy: 0.8360849022865295\n",
            "Final Test Accuracy: 0.8528891801834106\n",
            "Final Test Accuracy: 0.8537735939025879\n",
            "Final Test Accuracy: 0.8502358794212341\n",
            "Final Test Accuracy: 0.833431601524353\n",
            "Final Test Accuracy: 0.8434551954269409\n",
            "Final Test Accuracy: 0.8328419923782349\n",
            "Final Test Accuracy: 0.8390330076217651\n",
            "Final Test Accuracy: 0.8390330076217651\n",
            "Final Test Accuracy: 0.8319575786590576\n",
            "Final Test Accuracy: 0.8390330076217651\n",
            "Final Test Accuracy: 0.828125\n",
            "Final Test Accuracy: 0.8337264060974121\n",
            "Final Test Accuracy: 0.833136796951294\n",
            "Final Test Accuracy: 0.8363797068595886\n",
            "Final Test Accuracy: 0.8452240824699402\n",
            "Final Test Accuracy: 0.8375589847564697\n",
            "Final Test Accuracy: 0.8416863083839417\n",
            "Final Test Accuracy: 0.84375\n",
            "Final Test Accuracy: 0.829009473323822\n",
            "Final Test Accuracy: 0.8611438870429993\n",
            "Final Test Accuracy: 0.8307783007621765\n",
            "Final Test Accuracy: 0.8328419923782349\n",
            "Final Test Accuracy: 0.8307783007621765\n",
            "Final Test Accuracy: 0.8378537893295288\n",
            "Final Test Accuracy: 0.8310731053352356\n",
            "Final Test Accuracy: 0.8399174809455872\n",
            "Final Test Accuracy: 0.8455188870429993\n",
            "Final Test Accuracy: 0.8337264060974121\n",
            "Final Test Accuracy: 0.8390330076217651\n",
            "Final Test Accuracy: 0.8381485939025879\n",
            "Final Test Accuracy: 0.8431603908538818\n",
            "Final Test Accuracy: 0.833431601524353\n",
            "Final Test Accuracy: 0.8363797068595886\n",
            "Final Test Accuracy: 0.8319575786590576\n",
            "Final Test Accuracy: 0.8313679099082947\n",
            "Final Test Accuracy: 0.8284198045730591\n",
            "Final Test Accuracy: 0.8629127740859985\n",
            "Final Test Accuracy: 0.8328419923782349\n",
            "Final Test Accuracy: 0.8325471878051758\n",
            "Final Test Accuracy: 0.8354952931404114\n",
            "Avg test accuracy:  0.8382901054620743\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "open = np.load(\"/content/gdrive/My Drive/Research_Paper/Training/normalized-training-open-64ch.npy\")\n",
        "closed = np.load(\"/content/gdrive/My Drive/Research_Paper/Training/normalized-training-closed-64ch.npy\")\n",
        "\n",
        "test_100_accuracies = []\n",
        "\n",
        "for i in range(100):\n",
        "  accuracy = train(open, closed, True)\n",
        "  test_100_accuracies.append(accuracy)\n",
        "\n",
        "print(\"Avg test accuracy: \", np.mean(test_100_accuracies))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lE3DjcjDYxdq",
        "outputId": "86f104e3-ffe4-465b-f74c-3f3ba2cb4edc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Test Accuracy: 0.6371527910232544\n",
            "Final Test Accuracy: 0.6440972089767456\n",
            "Final Test Accuracy: 0.514756977558136\n",
            "Final Test Accuracy: 0.6493055820465088\n",
            "Final Test Accuracy: 0.6388888955116272\n",
            "Final Test Accuracy: 0.6032986044883728\n",
            "Final Test Accuracy: 0.6597222089767456\n",
            "Final Test Accuracy: 0.6032986044883728\n",
            "Final Test Accuracy: 0.65625\n",
            "Final Test Accuracy: 0.6432291865348816\n",
            "Final Test Accuracy: 0.6605902910232544\n",
            "Final Test Accuracy: 0.6597222089767456\n",
            "Final Test Accuracy: 0.6336805820465088\n",
            "Final Test Accuracy: 0.6371527910232544\n",
            "Final Test Accuracy: 0.6423611044883728\n",
            "Final Test Accuracy: 0.6597222089767456\n",
            "Final Test Accuracy: 0.6215277910232544\n",
            "Final Test Accuracy: 0.6380208134651184\n",
            "Final Test Accuracy: 0.6545138955116272\n",
            "Final Test Accuracy: 0.6432291865348816\n",
            "Final Test Accuracy: 0.6510416865348816\n",
            "Final Test Accuracy: 0.6736111044883728\n",
            "Final Test Accuracy: 0.6293402910232544\n",
            "Final Test Accuracy: 0.6467013955116272\n",
            "Final Test Accuracy: 0.6432291865348816\n",
            "Final Test Accuracy: 0.6579861044883728\n",
            "Final Test Accuracy: 0.6067708134651184\n",
            "Final Test Accuracy: 0.663194477558136\n",
            "Final Test Accuracy: 0.6189236044883728\n",
            "Final Test Accuracy: 0.6414930820465088\n",
            "Final Test Accuracy: 0.6302083134651184\n",
            "Final Test Accuracy: 0.5251736044883728\n",
            "Final Test Accuracy: 0.6145833134651184\n",
            "Final Test Accuracy: 0.65625\n",
            "Final Test Accuracy: 0.6180555820465088\n",
            "Final Test Accuracy: 0.6458333134651184\n",
            "Final Test Accuracy: 0.6519097089767456\n",
            "Final Test Accuracy: 0.6354166865348816\n",
            "Final Test Accuracy: 0.6449652910232544\n",
            "Final Test Accuracy: 0.6527777910232544\n",
            "Final Test Accuracy: 0.5902777910232544\n",
            "Final Test Accuracy: 0.6597222089767456\n",
            "Final Test Accuracy: 0.6041666865348816\n",
            "Final Test Accuracy: 0.6302083134651184\n",
            "Final Test Accuracy: 0.6258680820465088\n",
            "Final Test Accuracy: 0.6258680820465088\n",
            "Final Test Accuracy: 0.6545138955116272\n",
            "Final Test Accuracy: 0.647569477558136\n",
            "Final Test Accuracy: 0.5503472089767456\n",
            "Final Test Accuracy: 0.6345486044883728\n",
            "Final Test Accuracy: 0.6519097089767456\n",
            "Final Test Accuracy: 0.6310763955116272\n",
            "Final Test Accuracy: 0.6501736044883728\n",
            "Final Test Accuracy: 0.6571180820465088\n",
            "Final Test Accuracy: 0.6102430820465088\n",
            "Final Test Accuracy: 0.6597222089767456\n",
            "Final Test Accuracy: 0.6536458134651184\n",
            "Final Test Accuracy: 0.6614583134651184\n",
            "Final Test Accuracy: 0.6371527910232544\n",
            "Final Test Accuracy: 0.6675347089767456\n",
            "Final Test Accuracy: 0.6440972089767456\n",
            "Final Test Accuracy: 0.6519097089767456\n",
            "Final Test Accuracy: 0.6414930820465088\n",
            "Final Test Accuracy: 0.6605902910232544\n",
            "Final Test Accuracy: 0.5529513955116272\n",
            "Final Test Accuracy: 0.6206597089767456\n",
            "Final Test Accuracy: 0.6527777910232544\n",
            "Final Test Accuracy: 0.6605902910232544\n",
            "Final Test Accuracy: 0.577256977558136\n",
            "Final Test Accuracy: 0.65625\n",
            "Final Test Accuracy: 0.6484375\n",
            "Final Test Accuracy: 0.6640625\n",
            "Final Test Accuracy: 0.647569477558136\n",
            "Final Test Accuracy: 0.6432291865348816\n",
            "Final Test Accuracy: 0.639756977558136\n",
            "Final Test Accuracy: 0.6501736044883728\n",
            "Final Test Accuracy: 0.655381977558136\n",
            "Final Test Accuracy: 0.625\n",
            "Final Test Accuracy: 0.6657986044883728\n",
            "Final Test Accuracy: 0.6484375\n",
            "Final Test Accuracy: 0.6597222089767456\n",
            "Final Test Accuracy: 0.6510416865348816\n",
            "Final Test Accuracy: 0.6197916865348816\n",
            "Final Test Accuracy: 0.6527777910232544\n",
            "Final Test Accuracy: 0.6284722089767456\n",
            "Final Test Accuracy: 0.639756977558136\n",
            "Final Test Accuracy: 0.6440972089767456\n",
            "Final Test Accuracy: 0.6414930820465088\n",
            "Final Test Accuracy: 0.585069477558136\n",
            "Final Test Accuracy: 0.6380208134651184\n",
            "Final Test Accuracy: 0.6284722089767456\n",
            "Final Test Accuracy: 0.6605902910232544\n",
            "Final Test Accuracy: 0.6510416865348816\n",
            "Final Test Accuracy: 0.6458333134651184\n",
            "Final Test Accuracy: 0.6597222089767456\n",
            "Final Test Accuracy: 0.639756977558136\n",
            "Final Test Accuracy: 0.6519097089767456\n",
            "Final Test Accuracy: 0.6362847089767456\n",
            "Final Test Accuracy: 0.625\n",
            "Final Test Accuracy: 0.6458333134651184\n",
            "Avg test accuracy:  0.637222226858139\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "\n",
        "real = [0.6510416865348816, 0.6579861044883728, 0.6571180820465088, 0.65625, 0.6215277910232544, 0.6684027910232544, 0.6605902910232544, 0.6380208134651184, 0.6267361044883728, 0.6449652910232544, 0.6354166865348816, 0.6640625, 0.6484375, 0.6362847089767456, 0.65625, 0.6701388955116272, 0.6215277910232544, 0.663194477558136, 0.647569477558136, 0.6501736044883728, 0.6605902910232544, 0.6284722089767456, 0.6414930820465088, 0.6579861044883728, 0.6449652910232544, 0.6449652910232544, 0.6440972089767456, 0.5998263955116272, 0.6571180820465088, 0.6510416865348816, 0.6510416865348816, 0.6371527910232544, 0.647569477558136, 0.6223958134651184, 0.6388888955116272, 0.6258680820465088, 0.6145833134651184, 0.624131977558136, 0.6432291865348816, 0.655381977558136, 0.6493055820465088, 0.6293402910232544, 0.5703125, 0.5390625, 0.6493055820465088, 0.6467013955116272, 0.655381977558136, 0.6493055820465088, 0.6336805820465088, 0.6328125, 0.6458333134651184, 0.6701388955116272, 0.640625, 0.6579861044883728, 0.6232638955116272, 0.609375, 0.6345486044883728, 0.6284722089767456, 0.5711805820465088, 0.6588541865348816, 0.6684027910232544, 0.6588541865348816, 0.6501736044883728, 0.6432291865348816, 0.65625, 0.6657986044883728, 0.6258680820465088, 0.65625, 0.6032986044883728, 0.639756977558136, 0.6493055820465088, 0.6692708134651184, 0.6597222089767456, 0.6336805820465088, 0.6432291865348816, 0.6467013955116272, 0.6501736044883728, 0.5885416865348816, 0.6440972089767456, 0.6545138955116272, 0.6414930820465088, 0.592881977558136, 0.6336805820465088, 0.6423611044883728, 0.6458333134651184, 0.6614583134651184, 0.6519097089767456, 0.6232638955116272, 0.640625, 0.6458333134651184, 0.6440972089767456, 0.6302083134651184, 0.6362847089767456, 0.6432291865348816, 0.639756977558136, 0.6458333134651184, 0.6345486044883728, 0.6536458134651184, 0.6293402910232544, 0.631944477558136]\n",
        "real_generated = [0.8307783007621765, 0.8307783007621765, 0.8605542778968811, 0.8346108794212341, 0.8337264060974121, 0.8354952931404114, 0.8555424809455872, 0.8428655862808228, 0.829009473323822, 0.8307783007621765, 0.8337264060974121, 0.8325471878051758, 0.838738203048706, 0.8534787893295288, 0.8266509771347046, 0.8419811725616455, 0.8584905862808228, 0.8357900977134705, 0.8275353908538818, 0.8293042778968811, 0.839327871799469, 0.8295990824699402, 0.8354952931404114, 0.8416863083839417, 0.8605542778968811, 0.8307783007621765, 0.8452240824699402, 0.8405070900917053, 0.8461084961891174, 0.8434551954269409, 0.8349056839942932, 0.8354952931404114, 0.8605542778968811, 0.8316627740859985, 0.8328419923782349, 0.8360849022865295, 0.8363797068595886, 0.8378537893295288, 0.8278301954269409, 0.834316074848175, 0.833136796951294, 0.8316627740859985, 0.8304834961891174, 0.8723466992378235, 0.8522995114326477, 0.8449292778968811, 0.8337264060974121, 0.8340212106704712, 0.8328419923782349, 0.8328419923782349, 0.8434551954269409, 0.8410966992378235, 0.8301886916160583, 0.8531839847564697, 0.8316627740859985, 0.833431601524353, 0.823113203048706, 0.8301886916160583, 0.8316627740859985, 0.8360849022865295, 0.8528891801834106, 0.8537735939025879, 0.8502358794212341, 0.833431601524353, 0.8434551954269409, 0.8328419923782349, 0.8390330076217651, 0.8390330076217651, 0.8319575786590576, 0.8390330076217651, 0.828125, 0.8337264060974121, 0.833136796951294, 0.8363797068595886, 0.8452240824699402, 0.8375589847564697, 0.8416863083839417, 0.84375, 0.829009473323822, 0.8611438870429993, 0.8307783007621765, 0.8328419923782349, 0.8307783007621765, 0.8378537893295288, 0.8310731053352356, 0.8399174809455872, 0.8455188870429993, 0.8337264060974121, 0.8390330076217651, 0.8381485939025879, 0.8431603908538818, 0.833431601524353, 0.8363797068595886, 0.8319575786590576, 0.8313679099082947, 0.8284198045730591, 0.8629127740859985, 0.8328419923782349, 0.8325471878051758, 0.8354952931404114]\n",
        "real_real = [0.8130896091461182, 0.8157429099082947, 0.8136792778968811, 0.8139740824699402, 0.8083726763725281, 0.8116155862808228, 0.797759473323822, 0.8328419923782349, 0.7385023832321167, 0.7535377740859985, 0.8181014060974121, 0.8284198045730591, 0.7573702931404114, 0.8039504885673523, 0.822818398475647, 0.813384473323822, 0.8027712106704712, 0.817806601524353, 0.8027712106704712, 0.8310731053352356, 0.8151533007621765, 0.8051297068595886, 0.8293042778968811, 0.7735849022865295, 0.8254716992378235, 0.8222287893295288, 0.8248820900917053, 0.5386202931404114, 0.8402122855186462, 0.813384473323822, 0.803066074848175, 0.739681601524353, 0.8219339847564697, 0.8104363083839417, 0.8095518946647644, 0.823113203048706, 0.8269457817077637, 0.7556014060974121, 0.8275353908538818, 0.8110259771347046, 0.8493514060974121, 0.8113207817077637, 0.822818398475647, 0.8163325786590576, 0.698113203048706, 0.801886796951294, 0.8057193756103516, 0.8015919923782349, 0.8481721878051758, 0.828125, 0.7402712106704712, 0.7582547068595886, 0.7614976763725281, 0.7511792778968811, 0.7455778121948242, 0.8057193756103516, 0.8092570900917053, 0.8104363083839417, 0.8337264060974121, 0.8045400977134705, 0.8145636916160583, 0.8045400977134705, 0.8169221878051758, 0.823113203048706, 0.8095518946647644, 0.822818398475647, 0.807193398475647, 0.7986438870429993, 0.8310731053352356, 0.78125, 0.8033608794212341, 0.8216391801834106, 0.8319575786590576, 0.8352004885673523, 0.8183962106704712, 0.8125, 0.8172169923782349, 0.8107311725616455, 0.7956957817077637, 0.8119103908538818, 0.8104363083839417, 0.8225235939025879, 0.8119103908538818, 0.8166273832321167, 0.8313679099082947, 0.8275353908538818, 0.8248820900917053, 0.8360849022865295, 0.8172169923782349, 0.829009473323822, 0.8045400977134705, 0.8139740824699402, 0.8066037893295288, 0.8095518946647644, 0.7594339847564697, 0.8057193756103516, 0.8242924809455872, 0.8101415038108826, 0.8172169923782349, 0.8272405862808228]\n",
        "\n",
        "# real vs generated/real\n",
        "t_statistic, p_value = stats.ttest_rel(real, real_generated)\n",
        "print(p_value)\n",
        "\n",
        "# real vs realx2\n",
        "t_statistic, p_value = stats.ttest_rel(real_real, real_generated)\n",
        "print(p_value)\n",
        "\n"
      ],
      "metadata": {
        "id": "wGDq_ReTYyYL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "674a4829-ab96-4d35-a3da-76fac9607256"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9.997838508142085e-94\n",
            "1.924721611940723e-13\n"
          ]
        }
      ]
    }
  ]
}