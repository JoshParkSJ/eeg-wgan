{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfvoSY4W72Kv",
        "outputId": "9f655a56-83f6-4122-9142-06ce44b32c12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "hello GPU\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from abc import ABC, abstractmethod\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import seaborn as sns\n",
        "from scipy.linalg import sqrtm\n",
        "import os\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  print(\"hello GPU\")\n",
        "else:\n",
        "  print(\"sadge\")\n",
        "\n",
        "\n",
        "class BaseModel(nn.Module, ABC):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    @abstractmethod\n",
        "    def forward(self, x, interm=False):\n",
        "        pass\n",
        "\n",
        "    @property\n",
        "    def device(self):\n",
        "        return next(self.parameters()).device\n",
        "\n",
        "    def restore_checkpoint(self, ckpt_file, optimizer=None):\n",
        "        if not ckpt_file:\n",
        "            raise ValueError(\"No checkpoint file to be restored.\")\n",
        "\n",
        "        try:\n",
        "            ckpt_dict = torch.load(ckpt_file)\n",
        "        except RuntimeError:\n",
        "            ckpt_dict = torch.load(ckpt_file,\n",
        "                                   map_location=lambda storage, loc: storage)\n",
        "\n",
        "        # Restore model weights\n",
        "        self.load_state_dict(ckpt_dict['model_state_dict'])\n",
        "\n",
        "        # Restore optimizer status if existing. Evaluation doesn't need this\n",
        "        if optimizer:\n",
        "            optimizer.load_state_dict(ckpt_dict['optimizer_state_dict'])\n",
        "\n",
        "        # Return global step\n",
        "        return ckpt_dict['global_step']\n",
        "\n",
        "    def save_checkpoint(self,\n",
        "                        directory,\n",
        "                        global_step,\n",
        "                        optimizer=None,\n",
        "                        name=None):\n",
        "        # Create directory to save to\n",
        "        if not os.path.exists(directory):\n",
        "            os.makedirs(directory)\n",
        "\n",
        "        # Build checkpoint dict to save.\n",
        "        ckpt_dict = {\n",
        "            'model_state_dict': self.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict() if optimizer is not None else None,\n",
        "            'global_step': global_step\n",
        "        }\n",
        "\n",
        "        # Save the file with specific name\n",
        "        if name is None:\n",
        "            name = \"{}_{}_steps.pth\".format(\n",
        "                os.path.basename(directory),  # netD or netG\n",
        "                global_step)\n",
        "\n",
        "        torch.save(ckpt_dict, os.path.join(directory, name))\n",
        "\n",
        "    def count_params(self):\n",
        "        num_total_params = sum(p.numel() for p in self.parameters())\n",
        "        num_trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "        return num_total_params, num_trainable_params\n",
        "\n",
        "\n",
        "class DBlock(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 out_channels,\n",
        "                 kernel_size=3,\n",
        "                 stride=1,\n",
        "                 padding=0,\n",
        "                 downsample=False):\n",
        "        super().__init__()\n",
        "        self.downsample = downsample\n",
        "        self.c1 = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding)\n",
        "        self.activation = nn.LeakyReLU(0.2)\n",
        "\n",
        "        nn.init.normal_(self.c1.weight.data, 0.0, 0.02)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = x\n",
        "        h = self.c1(h)\n",
        "        h = self.activation(h)\n",
        "        if self.downsample:\n",
        "            h = F.avg_pool1d(h, 2)\n",
        "        return h\n",
        "\n",
        "class Discriminator(BaseModel):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__()\n",
        "        self.count = 0\n",
        "        self.errD_array = []\n",
        "        self.bce = nn.BCELoss()\n",
        "        self.ndf = 100\n",
        "\n",
        "        self.sblock1 = DBlock(64, self.ndf, kernel_size=9, stride=1, padding=0, downsample=False)\n",
        "        self.sblock2 = DBlock(self.ndf, self.ndf, kernel_size=9, stride=1, padding=0, downsample=True)\n",
        "        self.sblock3 = DBlock(self.ndf, self.ndf, kernel_size=9, stride=1, padding=0, downsample=False)\n",
        "        self.sblock4 = DBlock(self.ndf, self.ndf, kernel_size=9, stride=1, padding=0, downsample=True)\n",
        "        self.sblock5 = DBlock(self.ndf, self.ndf, kernel_size=9, stride=1, padding=0, downsample=False)\n",
        "        self.sblock6 = DBlock(self.ndf, self.ndf, kernel_size=9, stride=1, padding=0, downsample=True)\n",
        "        self.sblock7 = DBlock(self.ndf, self.ndf, kernel_size=9, stride=1, padding=0, downsample=False)\n",
        "        self.sblock8 = DBlock(self.ndf, self.ndf, kernel_size=9, stride=1, padding=0, downsample=True)\n",
        "        self.sblock9 = DBlock(self.ndf, self.ndf, kernel_size=9, stride=1, padding=0, downsample=False)\n",
        "        self.sblock10 = DBlock(self.ndf, self.ndf, kernel_size=9, stride=1, padding=0, downsample=True)\n",
        "        self.sblock11 = DBlock(self.ndf, self.ndf, kernel_size=9, stride=1, padding=0, downsample=False)\n",
        "        self.sblock12 = DBlock(self.ndf, self.ndf, kernel_size=9, stride=1, padding=0, downsample=True)\n",
        "        self.c = nn.Conv1d(self.ndf, 64, 1, 1, 0)\n",
        "        self.end = nn.Linear(33, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        nn.init.normal_(self.c.weight.data, 0.0, 0.02)\n",
        "        nn.init.normal_(self.end.weight.data, 0.0, 0.02)\n",
        "\n",
        "    def forward(self, x, interm=False):\n",
        "        x = x.float()\n",
        "        h = self.sblock1(x)\n",
        "        h = self.sblock2(h)\n",
        "        h = self.sblock3(h)\n",
        "        h = self.sblock4(h)\n",
        "        h = self.sblock5(h)\n",
        "        h = self.sblock6(h)\n",
        "        h = self.sblock7(h)\n",
        "        h = self.sblock8(h)\n",
        "        h = self.sblock9(h)\n",
        "        h = self.sblock10(h)\n",
        "        h = self.sblock11(h)\n",
        "        h = self.sblock12(h)\n",
        "        h = self.c(h)\n",
        "\n",
        "        interm_out = self.end(h)\n",
        "        if interm:\n",
        "            return interm_out\n",
        "        h = interm_out\n",
        "\n",
        "        h = self.sigmoid(h)\n",
        "        return h.view(h.shape[0], 64)\n",
        "\n",
        "    def compute_loss(self, output, actual):\n",
        "        return self.bce(output, actual)\n",
        "\n",
        "def train(open, closed, dupe=False):\n",
        "  device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
        "  netD = Discriminator().to(device)\n",
        "  optimizer = torch.optim.Adam(netD.parameters(), 0.0001, (0.5, 0.99))\n",
        "\n",
        "  train_lossD = []\n",
        "  train_accuracyD = []\n",
        "  test_lossD = []\n",
        "  test_accuracyD = []\n",
        "\n",
        "  # Split data into training and testing sets\n",
        "  train_open, test_open = train_test_split(open, test_size=0.2, random_state=42)\n",
        "  train_closed, test_closed = train_test_split(closed, test_size=0.2, random_state=42)\n",
        "\n",
        "  if (dupe):\n",
        "      train_open = np.concatenate((train_open, train_open))\n",
        "      train_closed = np.concatenate((train_closed, train_closed))\n",
        "\n",
        "  # Move data to tensors and to device\n",
        "  train_open = torch.tensor(train_open).float().to(device)\n",
        "  train_closed = torch.tensor(train_closed).float().to(device)\n",
        "  test_open = torch.tensor(test_open).float().to(device)\n",
        "  test_closed = torch.tensor(test_closed).float().to(device)\n",
        "\n",
        "  for epoch in range(100):\n",
        "    # Training\n",
        "    netD.zero_grad()\n",
        "\n",
        "    train_output_open = netD.forward(train_open)\n",
        "    train_output_closed = netD.forward(train_closed)\n",
        "\n",
        "    train_vals_open = torch.full(train_output_open.shape, 1.0, dtype=torch.float, device=device)\n",
        "    train_vals_closed = torch.full(train_output_closed.shape, 0.0, dtype=torch.float, device=device)\n",
        "\n",
        "    train_pred_labels = torch.cat((train_output_open, train_output_closed))\n",
        "    train_actual_labels = torch.cat((train_vals_open, train_vals_closed))\n",
        "\n",
        "    train_accuracy = ((train_pred_labels > 0.5) == train_actual_labels).float().mean()\n",
        "    train_accuracyD.append(train_accuracy.item())\n",
        "\n",
        "    # Compute training loss\n",
        "    train_errD_real = netD.compute_loss(train_output_open, train_vals_open)\n",
        "    train_errD_fake = netD.compute_loss(train_output_closed, train_vals_closed)\n",
        "    train_errD = train_errD_real + train_errD_fake\n",
        "    train_errD.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    train_lossD.append(train_errD.item())\n",
        "\n",
        "    # Testing\n",
        "    with torch.no_grad():\n",
        "      test_output_open = netD.forward(test_open)\n",
        "      test_output_closed = netD.forward(test_closed)\n",
        "\n",
        "      # Compute classification accuracy\n",
        "      test_vals_open = torch.full(test_output_open.shape, 1.0, dtype=torch.float, device=device)\n",
        "      test_vals_closed = torch.full(test_output_closed.shape, 0.0, dtype=torch.float, device=device)\n",
        "\n",
        "      test_pred_labels = torch.cat((test_output_open, test_output_closed))\n",
        "      test_actual_labels = torch.cat((test_vals_open, test_vals_closed))\n",
        "\n",
        "      test_accuracy = ((test_pred_labels > 0.5) == test_actual_labels).float().mean()\n",
        "\n",
        "      # Bin guess into T/F for confusion matrix\n",
        "      test_output_open_cm = (test_output_open.mean(dim=1, keepdim=True) > 0.5).float()\n",
        "      test_output_closed_cm = (test_output_closed.mean(dim=1, keepdim=True) > 0.5).float()\n",
        "      test_vals_open_cm = (test_vals_open.mean(dim=1, keepdim=True) > 0.5).float()\n",
        "      test_vals_closed_cm = (test_vals_closed.mean(dim=1, keepdim=True) > 0.5).float()\n",
        "\n",
        "      # Compute testing loss\n",
        "      test_errD_real = netD.compute_loss(test_output_open, test_vals_open)\n",
        "      test_errD_fake = netD.compute_loss(test_output_closed, test_vals_closed)\n",
        "      test_errD = test_errD_real + test_errD_fake\n",
        "\n",
        "      test_accuracyD.append(test_accuracy.item())\n",
        "      test_lossD.append(test_errD.item())\n",
        "\n",
        "    # if epoch % 20 == 0:\n",
        "    #   print('Epoch', epoch)\n",
        "    #   print('Train Loss:', train_errD.item(), 'Test Loss:', test_errD.item())\n",
        "    #   print('Train Accuracy:', train_accuracy.item(), 'Test Accuracy:', test_accuracy.item())\n",
        "    #   plt.figure(figsize=(10,4))\n",
        "    #   plt.subplot(1, 2, 1)\n",
        "    #   plt.plot(train_lossD, label='Training Loss')\n",
        "    #   plt.plot(test_lossD, label='Testing Loss')\n",
        "    #   plt.legend()\n",
        "    #   plt.subplot(1, 2, 2)\n",
        "    #   plt.plot(train_accuracyD, label='Training Accuracy')\n",
        "    #   plt.plot(test_accuracyD, label='Testing Accuracy')\n",
        "    #   plt.legend()\n",
        "    #   plt.show()\n",
        "\n",
        "  print('Final Test Accuracy:', test_accuracy.item())\n",
        "  return torch.cat((test_output_open_cm, test_output_closed_cm)), torch.cat((test_vals_open_cm, test_vals_closed_cm)), netD, test_accuracy.item()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy\n",
        "from scipy.linalg import sqrtm\n",
        "from scipy.linalg import sqrtm\n",
        "from scipy import linalg\n",
        "import numpy as np\n",
        "\n",
        "open = np.load(\"/content/gdrive/My Drive/Research_Paper/Training/normalized-training-open-64ch.npy\")\n",
        "closed = np.load(\"/content/gdrive/My Drive/Research_Paper/Training/normalized-training-closed-64ch.npy\")\n",
        "open_generated = np.load(\"/content/gdrive/My Drive/Research_Paper/generated-data/generated-open-1.npy\")\n",
        "closed_generated = np.load(\"/content/gdrive/My Drive/Research_Paper/generated-data/generated-closed-1.npy\")\n",
        "\n",
        "# def calculate_FID(real, fake):\n",
        "#     real_data = real.float().to('cuda')\n",
        "#     generated_data = fake.float().to('cuda')\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         act_real = netD(real_data, interm=True)\n",
        "#         act_generated = netD(generated_data, interm=True)\n",
        "\n",
        "#     # Reshaping the activations and treating each time-series data point as an individual instance\n",
        "#     act_real = act_real.reshape(-1, act_real.shape[-1])\n",
        "#     act_generated = act_generated.reshape(-1, act_generated.shape[-1])\n",
        "\n",
        "#     act_real = act_real.cpu().numpy()\n",
        "#     act_generated = act_generated.cpu().numpy()\n",
        "\n",
        "#     # Calculate mean and covariance statistics\n",
        "#     mu_real, sigma_real = act_real.mean(axis=0), np.cov(act_real, rowvar=False)\n",
        "#     mu_gen, sigma_gen = act_generated.mean(axis=0), np.cov(act_generated, rowvar=False)\n",
        "\n",
        "#     # Calculate sum squared difference between means\n",
        "#     ssdiff = np.sum((mu_real - mu_gen) ** 2.0)\n",
        "\n",
        "#     # Calculate sqrt of product between cov\n",
        "#     covmean, _ = scipy.linalg.sqrtm(sigma_real.dot(sigma_gen), disp=False)\n",
        "\n",
        "#     # Check and correct imaginary numbers from sqrt\n",
        "#     if np.iscomplexobj(covmean):\n",
        "#         covmean = covmean.real\n",
        "\n",
        "#     # Sum of the differences in covariance\n",
        "#     fid = ssdiff + np.trace(sigma_real + sigma_gen - 2.0 * covmean)\n",
        "#     # fid = fid / 10000\n",
        "#     print('FID Score: %.3f' % fid)\n",
        "\n",
        "\n",
        "\n",
        "def calculate_FID(real, fake, netD):\n",
        "    real_data = real.float().to('cuda')\n",
        "    generated_data = fake.float().to('cuda')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        act_real = netD(real_data, interm=True)\n",
        "        act_generated = netD(generated_data, interm=True)\n",
        "\n",
        "    # Reshaping the activations and treating each time-series data point as an individual instance\n",
        "    act_real = act_real.reshape(-1, act_real.shape[-1])\n",
        "    act_generated = act_generated.reshape(-1, act_generated.shape[-1])\n",
        "\n",
        "    act_real = act_real.cpu().numpy()\n",
        "    act_generated = act_generated.cpu().numpy()\n",
        "\n",
        "    # Calculate mean and covariance statistics\n",
        "    mu_real, sigma_real = act_real.mean(axis=0), np.cov(act_real, rowvar=False)\n",
        "    mu_gen, sigma_gen = act_generated.mean(axis=0), np.cov(act_generated, rowvar=False)\n",
        "\n",
        "    fid = calculate_frechet_distance(mu_real, sigma_real, mu_gen, sigma_gen)\n",
        "    return fid\n",
        "\n",
        "def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
        "    if mu1.shape != mu2.shape or sigma1.shape != sigma2.shape:\n",
        "        raise ValueError(\n",
        "            \"(mu1, sigma1) should have exactly the same shape as (mu2, sigma2).\"\n",
        "        )\n",
        "\n",
        "    mu1 = np.atleast_1d(mu1)\n",
        "    mu2 = np.atleast_1d(mu2)\n",
        "\n",
        "    sigma1 = np.atleast_2d(sigma1)\n",
        "    sigma2 = np.atleast_2d(sigma2)\n",
        "\n",
        "    diff = mu1 - mu2\n",
        "\n",
        "    # Product might be almost singular\n",
        "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
        "    if not np.isfinite(covmean).all():\n",
        "        print(\n",
        "            \"WARNING: fid calculation produces singular product; adding {} to diagonal of cov estimates\"\n",
        "            .format(eps))\n",
        "\n",
        "        offset = np.eye(sigma1.shape[0]) * eps\n",
        "        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n",
        "\n",
        "    # Numerical error might give slight imaginary component\n",
        "    if np.iscomplexobj(covmean):\n",
        "        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
        "            m = np.max(np.abs(covmean.imag))\n",
        "            raise ValueError(\"Imaginary component {}\".format(m))\n",
        "        covmean = covmean.real\n",
        "\n",
        "    tr_covmean = np.trace(covmean)\n",
        "\n",
        "    return diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2 * tr_covmean\n"
      ],
      "metadata": {
        "id": "jn13SaZYUvFz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.layers import Conv1D, GlobalAveragePooling1D\n",
        "# from tensorflow.keras import backend as K\n",
        "# import tensorflow as tf\n",
        "# import numpy as np\n",
        "# from scipy.linalg import sqrtm\n",
        "# from scipy import linalg\n",
        "# import numpy as np\n",
        "\n",
        "\n",
        "# def calculate_fidt(signals1, signals2, feature_extractor):\n",
        "\n",
        "#   # Convert Tensors to CUDA\n",
        "#   signals1 = signals1.cuda()\n",
        "#   signals2 = signals2.cuda()\n",
        "\n",
        "#   with torch.no_grad():\n",
        "#       act1 = feature_extractor(signals1, interm=True).reshape((signals1.shape[0], signals1.shape[1]))\n",
        "#       act2 = feature_extractor(signals2, interm=True).reshape((signals2.shape[0], signals2.shape[1]))\n",
        "\n",
        "#   act1 = act1.cpu().numpy()\n",
        "#   act2 = act2.cpu().numpy()\n",
        "\n",
        "#   # Calculate mean and covariance\n",
        "#   mu1, sigma1 = act1.mean(axis=0), np.cov(act1, rowvar=False)\n",
        "#   mu2, sigma2 = act2.mean(axis=0), np.cov(act2, rowvar=False)\n",
        "\n",
        "#   # Calculate sum of squares difference between means\n",
        "#   ssdiff = np.sum((mu1 - mu2)**2.0)\n",
        "\n",
        "#   # Calculate sqrt of product between cov\n",
        "#   covmean = sqrtm(sigma1.dot(sigma2))\n",
        "\n",
        "#   # Check and correct imaginary numbers from sqrt\n",
        "#   if np.iscomplexobj(covmean):\n",
        "#       covmean = covmean.real\n",
        "\n",
        "#   # Calculate score\n",
        "#   fid = ssdiff + np.trace(np.absolute(sigma1 + sigma2 - 2.0 * covmean))  # modified line\n",
        "#   return fid\n"
      ],
      "metadata": {
        "id": "j_Moxh8f8qvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate FID scores\n",
        "\n",
        "average 100 iterations"
      ],
      "metadata": {
        "id": "hMvYgEhSDNN8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "open_realfake = []\n",
        "closed_realfake = []\n",
        "open_realreal = []\n",
        "closed_realreal = []\n",
        "open_realnoise = []\n",
        "closed_realnoise = []\n",
        "\n",
        "\n",
        "for _ in range(100):\n",
        "  predicted_labels_synthetic_classifier, true_labels_synthetic_classifier, netD, accuracy = train(open, closed)\n",
        "\n",
        "  # open_realfake\n",
        "  real_data = torch.tensor(open).float()\n",
        "  generated_data = torch.tensor(open_generated).float()\n",
        "  open_realfake.append(calculate_FID(real_data, generated_data, netD)/10)\n",
        "\n",
        "  # closed_realfake\n",
        "  real_data = torch.tensor(closed).float()\n",
        "  generated_data = torch.tensor(closed_generated).float()\n",
        "  closed_realfake.append(calculate_FID(real_data, generated_data, netD))\n",
        "\n",
        "  # open_realreal\n",
        "  real_data = torch.tensor(open).float()\n",
        "  open_realreal.append(calculate_FID(real_data, real_data, netD))\n",
        "\n",
        "  # closed_realreal\n",
        "  real_data = torch.tensor(closed).float()\n",
        "  closed_realreal.append(calculate_FID(real_data, real_data, netD))\n",
        "\n",
        "  # noise open\n",
        "  real_data = torch.tensor(open).float()\n",
        "  noise_data = torch.randn((42, 64, 3152)).float()\n",
        "  open_realnoise.append(calculate_FID(real_data, noise_data, netD))\n",
        "\n",
        "  # noise closed\n",
        "  real_data = torch.tensor(closed).float()\n",
        "  noise_data = torch.randn((45, 64, 3152)).float()\n",
        "  closed_realnoise.append(calculate_FID(real_data, noise_data, netD))\n",
        "\n",
        "print(\"open_realfake: \", np.mean(open_realfake))\n",
        "print(\"closed_realfake: \", np.mean(closed_realfake))\n",
        "print(\"open_realreal: \", np.mean(open_realreal))\n",
        "print(\"closed_realreal: \", np.mean(closed_realreal))\n",
        "print(\"open_realnoise: \", np.mean(open_realnoise))\n",
        "print(\"closed_realnoise: \", np.mean(closed_realnoise))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hPBjYlKCs6c",
        "outputId": "b8d2584f-8810-44e3-d20f-6f8fc65b5c12"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Test Accuracy: 0.7777777910232544\n",
            "Final Test Accuracy: 0.5529513955116272\n",
            "Final Test Accuracy: 0.8012152910232544\n",
            "Final Test Accuracy: 0.7769097089767456\n",
            "Final Test Accuracy: 0.7248263955116272\n",
            "Final Test Accuracy: 0.8315972089767456\n",
            "Final Test Accuracy: 0.8307291865348816\n",
            "Final Test Accuracy: 0.7222222089767456\n",
            "Final Test Accuracy: 0.8888888955116272\n",
            "Final Test Accuracy: 0.7222222089767456\n",
            "Final Test Accuracy: 0.7395833134651184\n",
            "Final Test Accuracy: 0.7777777910232544\n",
            "Final Test Accuracy: 0.7222222089767456\n",
            "Final Test Accuracy: 0.7890625\n",
            "Final Test Accuracy: 0.835069477558136\n",
            "Final Test Accuracy: 0.7222222089767456\n",
            "Final Test Accuracy: 0.8333333134651184\n",
            "Final Test Accuracy: 0.8333333134651184\n",
            "Final Test Accuracy: 0.8888888955116272\n",
            "Final Test Accuracy: 0.8298611044883728\n",
            "Final Test Accuracy: 0.7196180820465088\n",
            "Final Test Accuracy: 0.8333333134651184\n",
            "Final Test Accuracy: 0.8810763955116272\n",
            "Final Test Accuracy: 0.8333333134651184\n",
            "Final Test Accuracy: 0.734375\n",
            "Final Test Accuracy: 0.8333333134651184\n",
            "Final Test Accuracy: 0.7795138955116272\n",
            "Final Test Accuracy: 0.8888888955116272\n",
            "Final Test Accuracy: 0.7925347089767456\n",
            "Final Test Accuracy: 0.7847222089767456\n",
            "Final Test Accuracy: 0.8333333134651184\n",
            "Final Test Accuracy: 0.7795138955116272\n",
            "Final Test Accuracy: 0.7282986044883728\n",
            "Final Test Accuracy: 0.8888888955116272\n",
            "Final Test Accuracy: 0.7222222089767456\n",
            "Final Test Accuracy: 0.8559027910232544\n",
            "Final Test Accuracy: 0.7777777910232544\n",
            "Final Test Accuracy: 0.8333333134651184\n",
            "Final Test Accuracy: 0.8298611044883728\n",
            "Final Test Accuracy: 0.8888888955116272\n",
            "Final Test Accuracy: 0.8333333134651184\n",
            "Final Test Accuracy: 0.7213541865348816\n",
            "Final Test Accuracy: 0.8333333134651184\n",
            "Final Test Accuracy: 0.78125\n",
            "Final Test Accuracy: 0.875\n",
            "Final Test Accuracy: 0.7222222089767456\n",
            "Final Test Accuracy: 0.8333333134651184\n",
            "Final Test Accuracy: 0.780381977558136\n",
            "Final Test Accuracy: 0.8333333134651184\n",
            "Final Test Accuracy: 0.6840277910232544\n",
            "Final Test Accuracy: 0.7630208134651184\n",
            "Final Test Accuracy: 0.8229166865348816\n",
            "Final Test Accuracy: 0.7795138955116272\n",
            "Final Test Accuracy: 0.7777777910232544\n",
            "Final Test Accuracy: 0.7604166865348816\n",
            "Final Test Accuracy: 0.811631977558136\n",
            "Final Test Accuracy: 0.8333333134651184\n",
            "Final Test Accuracy: 0.8333333134651184\n",
            "Final Test Accuracy: 0.7552083134651184\n",
            "Final Test Accuracy: 0.780381977558136\n",
            "Final Test Accuracy: 0.8255208134651184\n",
            "Final Test Accuracy: 0.8888888955116272\n",
            "Final Test Accuracy: 0.7265625\n",
            "Final Test Accuracy: 0.8333333134651184\n",
            "Final Test Accuracy: 0.7786458134651184\n",
            "Final Test Accuracy: 0.8888888955116272\n",
            "Final Test Accuracy: 0.780381977558136\n",
            "Final Test Accuracy: 0.7230902910232544\n",
            "Final Test Accuracy: 0.8072916865348816\n",
            "Final Test Accuracy: 0.7222222089767456\n",
            "Final Test Accuracy: 0.6666666865348816\n",
            "Final Test Accuracy: 0.7560763955116272\n",
            "Final Test Accuracy: 0.8333333134651184\n",
            "Final Test Accuracy: 0.7777777910232544\n",
            "Final Test Accuracy: 0.8342013955116272\n",
            "Final Test Accuracy: 0.7777777910232544\n",
            "Final Test Accuracy: 0.7439236044883728\n",
            "Final Test Accuracy: 0.780381977558136\n",
            "Final Test Accuracy: 0.7170138955116272\n",
            "Final Test Accuracy: 0.6354166865348816\n",
            "Final Test Accuracy: 0.8333333134651184\n",
            "Final Test Accuracy: 0.8810763955116272\n",
            "Final Test Accuracy: 0.6744791865348816\n",
            "Final Test Accuracy: 0.7690972089767456\n",
            "Final Test Accuracy: 0.7222222089767456\n",
            "Final Test Accuracy: 0.8333333134651184\n",
            "Final Test Accuracy: 0.8888888955116272\n",
            "Final Test Accuracy: 0.8888888955116272\n",
            "Final Test Accuracy: 0.8333333134651184\n",
            "Final Test Accuracy: 0.7222222089767456\n",
            "Final Test Accuracy: 0.8220486044883728\n",
            "Final Test Accuracy: 0.7855902910232544\n",
            "Final Test Accuracy: 0.944444477558136\n",
            "Final Test Accuracy: 0.7578125\n",
            "Final Test Accuracy: 0.780381977558136\n",
            "Final Test Accuracy: 0.8333333134651184\n",
            "Final Test Accuracy: 0.8888888955116272\n",
            "Final Test Accuracy: 0.703125\n",
            "Final Test Accuracy: 0.7777777910232544\n",
            "Final Test Accuracy: 0.8237847089767456\n",
            "open_realfake:  1.3447035224071469\n",
            "closed_realfake:  11.565017997179773\n",
            "open_realreal:  0.0\n",
            "closed_realreal:  0.0\n",
            "open_realnoise:  4866.598328576814\n",
            "closed_realnoise:  1083.099654834708\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pre_accuracy = [0.7777777910232544, 0.5529513955116272, 0.8012152910232544, 0.7769097089767456, 0.7248263955116272, 0.8315972089767456, 0.8307291865348816, 0.7222222089767456, 0.8888888955116272, 0.7222222089767456, 0.7395833134651184, 0.7777777910232544, 0.7222222089767456, 0.7890625, 0.835069477558136, 0.7222222089767456, 0.8333333134651184, 0.8333333134651184, 0.8888888955116272, 0.8298611044883728, 0.7196180820465088, 0.8333333134651184, 0.8810763955116272, 0.8333333134651184, 0.734375, 0.8333333134651184, 0.7795138955116272, 0.8888888955116272, 0.7925347089767456, 0.7847222089767456, 0.8333333134651184, 0.7795138955116272, 0.7282986044883728, 0.8888888955116272, 0.7222222089767456, 0.8559027910232544, 0.7777777910232544, 0.8333333134651184, 0.8298611044883728, 0.8888888955116272, 0.8333333134651184, 0.7213541865348816, 0.8333333134651184, 0.78125, 0.875, 0.7222222089767456, 0.8333333134651184, 0.780381977558136, 0.8333333134651184, 0.6840277910232544, 0.7630208134651184, 0.8229166865348816, 0.7795138955116272, 0.7777777910232544, 0.7604166865348816, 0.811631977558136, 0.8333333134651184, 0.8333333134651184, 0.7552083134651184, 0.780381977558136, 0.8255208134651184, 0.8888888955116272, 0.7265625, 0.8333333134651184, 0.7786458134651184, 0.8888888955116272, 0.780381977558136, 0.7230902910232544, 0.8072916865348816, 0.7222222089767456, 0.6666666865348816, 0.7560763955116272, 0.8333333134651184, 0.7777777910232544, 0.8342013955116272, 0.7777777910232544, 0.7439236044883728, 0.780381977558136, 0.7170138955116272, 0.6354166865348816, 0.8333333134651184, 0.8810763955116272, 0.6744791865348816, 0.7690972089767456, 0.7222222089767456, 0.8333333134651184, 0.8888888955116272, 0.8888888955116272, 0.8333333134651184, 0.7222222089767456, 0.8220486044883728, 0.7855902910232544, 0.944444477558136, 0.7578125, 0.780381977558136, 0.8333333134651184, 0.8888888955116272, 0.703125, 0.7777777910232544, 0.8237847089767456]\n",
        "np.mean(pre_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-W_m9BXWTo4",
        "outputId": "133f5db5-c693-409e-9a09-561281748df2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7929600697755813"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Accuracy\n",
        "\n",
        "over 100 iterations"
      ],
      "metadata": {
        "id": "UjNSQZrEWHGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "open = np.load(\"/content/gdrive/My Drive/Research_Paper/Training/normalized-training-open-64ch.npy\")\n",
        "closed = np.load(\"/content/gdrive/My Drive/Research_Paper/Training/normalized-training-closed-64ch.npy\")\n",
        "open_generated1 = np.load(\"/content/gdrive/My Drive/Research_Paper/generated-data/generated-open-1.npy\")\n",
        "closed_generated1 = np.load(\"/content/gdrive/My Drive/Research_Paper/generated-data/generated-closed-1.npy\")\n",
        "open_generated = np.load(\"/content/gdrive/My Drive/Research_Paper/generated-data/generated-open-1.npy\")\n",
        "closed_generated = np.load(\"/content/gdrive/My Drive/Research_Paper/generated-data/generated-closed-1.npy\")\n",
        "\n",
        "open = np.concatenate((open, open_generated1))\n",
        "closed = np.concatenate((closed, closed_generated1))\n",
        "open = np.concatenate((open, open_generated))\n",
        "closed = np.concatenate((closed, closed_generated))\n",
        "test_100_accuracies = []\n",
        "\n",
        "for i in range(100):\n",
        "  predicted_labels_synthetic_classifier, true_labels_synthetic_classifier, netD, accuracy = train(open, closed)\n",
        "  test_100_accuracies.append(accuracy)\n",
        "\n",
        "print(\"Avg test accuracy: \", np.mean(test_100_accuracies))\n",
        "\n"
      ],
      "metadata": {
        "id": "F-n-YR4O6xvv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62212924-5dea-438c-dc42-c132ac45e2db"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Test Accuracy: 0.9610849022865295\n",
            "Final Test Accuracy: 0.9761202931404114\n",
            "Final Test Accuracy: 1.0\n",
            "Final Test Accuracy: 0.9811320900917053\n",
            "Final Test Accuracy: 0.9811320900917053\n",
            "Final Test Accuracy: 0.9451650977134705\n",
            "Final Test Accuracy: 0.9646226763725281\n",
            "Final Test Accuracy: 0.963443398475647\n",
            "Final Test Accuracy: 0.9516509771347046\n",
            "Final Test Accuracy: 0.9879127740859985\n",
            "Final Test Accuracy: 0.9433962106704712\n",
            "Final Test Accuracy: 0.9814268946647644\n",
            "Final Test Accuracy: 0.9543042778968811\n",
            "Final Test Accuracy: 0.9811320900917053\n",
            "Final Test Accuracy: 0.9265919923782349\n",
            "Final Test Accuracy: 0.9811320900917053\n",
            "Final Test Accuracy: 0.9811320900917053\n",
            "Final Test Accuracy: 1.0\n",
            "Final Test Accuracy: 0.9439858794212341\n",
            "Final Test Accuracy: 0.9811320900917053\n",
            "Final Test Accuracy: 0.9622641801834106\n",
            "Final Test Accuracy: 0.9504716992378235\n",
            "Final Test Accuracy: 0.9805424809455872\n",
            "Final Test Accuracy: 0.9622641801834106\n",
            "Final Test Accuracy: 0.9761202931404114\n",
            "Final Test Accuracy: 1.0\n",
            "Final Test Accuracy: 0.9463443756103516\n",
            "Final Test Accuracy: 0.9811320900917053\n",
            "Final Test Accuracy: 0.9625589847564697\n",
            "Final Test Accuracy: 0.9831957817077637\n",
            "Final Test Accuracy: 0.9245283007621765\n",
            "Final Test Accuracy: 0.9811320900917053\n",
            "Final Test Accuracy: 0.943691074848175\n",
            "Final Test Accuracy: 0.9345518946647644\n",
            "Final Test Accuracy: 0.9811320900917053\n",
            "Final Test Accuracy: 0.9622641801834106\n",
            "Final Test Accuracy: 0.9811320900917053\n",
            "Final Test Accuracy: 0.9802476763725281\n",
            "Final Test Accuracy: 0.948702871799469\n",
            "Final Test Accuracy: 0.9802476763725281\n",
            "Final Test Accuracy: 0.9625589847564697\n",
            "Final Test Accuracy: 1.0\n",
            "Final Test Accuracy: 0.9920400977134705\n",
            "Final Test Accuracy: 0.9811320900917053\n",
            "Final Test Accuracy: 0.9811320900917053\n",
            "Final Test Accuracy: 0.9619693756103516\n",
            "Final Test Accuracy: 0.9699292778968811\n",
            "Final Test Accuracy: 0.9811320900917053\n",
            "Final Test Accuracy: 0.9811320900917053\n",
            "Final Test Accuracy: 0.9811320900917053\n",
            "Final Test Accuracy: 0.9808372855186462\n",
            "Final Test Accuracy: 0.9622641801834106\n",
            "Final Test Accuracy: 0.9952830076217651\n",
            "Final Test Accuracy: 0.9811320900917053\n",
            "Final Test Accuracy: 0.9433962106704712\n",
            "Final Test Accuracy: 0.9628537893295288\n",
            "Final Test Accuracy: 0.9655070900917053\n",
            "Final Test Accuracy: 0.9811320900917053\n",
            "Final Test Accuracy: 0.9245283007621765\n",
            "Final Test Accuracy: 0.9622641801834106\n",
            "Final Test Accuracy: 0.9507665038108826\n",
            "Final Test Accuracy: 0.885318398475647\n",
            "Final Test Accuracy: 0.9811320900917053\n",
            "Final Test Accuracy: 0.9433962106704712\n",
            "Final Test Accuracy: 0.9814268946647644\n",
            "Final Test Accuracy: 1.0\n",
            "Final Test Accuracy: 0.9622641801834106\n",
            "Final Test Accuracy: 0.9811320900917053\n",
            "Final Test Accuracy: 0.9489976763725281\n",
            "Final Test Accuracy: 0.9622641801834106\n",
            "Final Test Accuracy: 0.989386796951294\n",
            "Final Test Accuracy: 0.9433962106704712\n",
            "Final Test Accuracy: 0.9610849022865295\n",
            "Final Test Accuracy: 0.9563679099082947\n",
            "Final Test Accuracy: 0.9622641801834106\n",
            "Final Test Accuracy: 0.9811320900917053\n",
            "Final Test Accuracy: 0.9811320900917053\n",
            "Final Test Accuracy: 0.9433962106704712\n",
            "Final Test Accuracy: 0.9811320900917053\n",
            "Final Test Accuracy: 0.9622641801834106\n",
            "Final Test Accuracy: 0.9622641801834106\n",
            "Final Test Accuracy: 0.9622641801834106\n",
            "Final Test Accuracy: 0.9873231053352356\n",
            "Final Test Accuracy: 0.9433962106704712\n",
            "Final Test Accuracy: 0.9935141801834106\n",
            "Final Test Accuracy: 0.9622641801834106\n",
            "Final Test Accuracy: 0.9811320900917053\n",
            "Final Test Accuracy: 0.9622641801834106\n",
            "Final Test Accuracy: 0.9811320900917053\n",
            "Final Test Accuracy: 0.958136796951294\n",
            "Final Test Accuracy: 0.947818398475647\n",
            "Final Test Accuracy: 0.9961674809455872\n",
            "Final Test Accuracy: 0.7612028121948242\n",
            "Final Test Accuracy: 1.0\n",
            "Final Test Accuracy: 0.9858490824699402\n",
            "Final Test Accuracy: 0.9811320900917053\n",
            "Final Test Accuracy: 0.9811320900917053\n",
            "Final Test Accuracy: 0.9811320900917053\n",
            "Final Test Accuracy: 0.9327830076217651\n",
            "Final Test Accuracy: 0.8464033007621765\n",
            "Avg test accuracy:  0.9652594476938248\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "open = np.load(\"/content/gdrive/My Drive/Research_Paper/Training/normalized-training-open-64ch.npy\")\n",
        "closed = np.load(\"/content/gdrive/My Drive/Research_Paper/Training/normalized-training-closed-64ch.npy\")\n",
        "open1 = np.load(\"/content/gdrive/My Drive/Research_Paper/Training/normalized-training-open-64ch.npy\")\n",
        "closed1 = np.load(\"/content/gdrive/My Drive/Research_Paper/Training/normalized-training-closed-64ch.npy\")\n",
        "\n",
        "test_100_accuracies = []\n",
        "\n",
        "for i in range(100):\n",
        "  predicted_labels_synthetic_classifier, true_labels_synthetic_classifier, netD, accuracy = train(open, closed, True)\n",
        "  test_100_accuracies.append(accuracy)\n",
        "\n",
        "print(\"Avg test accuracy: \", np.mean(test_100_accuracies))\n",
        "\n"
      ],
      "metadata": {
        "id": "oI5hvG_3Wjlr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae66650e-1958-4638-9f9d-1ef32b4783eb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Test Accuracy: 0.7777777910232544\n",
            "Final Test Accuracy: 0.8185763955116272\n",
            "Final Test Accuracy: 0.8333333134651184\n",
            "Final Test Accuracy: 0.8880208134651184\n",
            "Final Test Accuracy: 0.8333333134651184\n",
            "Final Test Accuracy: 0.8402777910232544\n",
            "Final Test Accuracy: 0.7777777910232544\n",
            "Final Test Accuracy: 0.8333333134651184\n",
            "Final Test Accuracy: 0.6892361044883728\n",
            "Final Test Accuracy: 0.8333333134651184\n",
            "Final Test Accuracy: 0.8880208134651184\n",
            "Final Test Accuracy: 0.7222222089767456\n",
            "Final Test Accuracy: 0.8246527910232544\n",
            "Final Test Accuracy: 0.8333333134651184\n",
            "Final Test Accuracy: 0.8333333134651184\n",
            "Final Test Accuracy: 0.7777777910232544\n",
            "Final Test Accuracy: 0.8333333134651184\n",
            "Final Test Accuracy: 0.8524305820465088\n",
            "Final Test Accuracy: 0.8333333134651184\n",
            "Final Test Accuracy: 0.7777777910232544\n",
            "Final Test Accuracy: 0.8307291865348816\n",
            "Final Test Accuracy: 0.8567708134651184\n",
            "Final Test Accuracy: 0.7760416865348816\n",
            "Final Test Accuracy: 0.6666666865348816\n",
            "Final Test Accuracy: 0.8185763955116272\n",
            "Final Test Accuracy: 0.7769097089767456\n",
            "Final Test Accuracy: 0.8888888955116272\n",
            "Final Test Accuracy: 0.7265625\n",
            "Final Test Accuracy: 0.7777777910232544\n",
            "Final Test Accuracy: 0.9383680820465088\n",
            "Final Test Accuracy: 0.7222222089767456\n",
            "Final Test Accuracy: 0.6666666865348816\n",
            "Final Test Accuracy: 0.7222222089767456\n",
            "Final Test Accuracy: 0.7230902910232544\n",
            "Final Test Accuracy: 0.936631977558136\n",
            "Final Test Accuracy: 0.7777777910232544\n",
            "Final Test Accuracy: 0.7222222089767456\n",
            "Final Test Accuracy: 0.8888888955116272\n",
            "Final Test Accuracy: 0.6675347089767456\n",
            "Final Test Accuracy: 0.8888888955116272\n",
            "Final Test Accuracy: 0.7777777910232544\n",
            "Final Test Accuracy: 0.8333333134651184\n",
            "Final Test Accuracy: 0.8333333134651184\n",
            "Final Test Accuracy: 0.8333333134651184\n",
            "Final Test Accuracy: 0.7717013955116272\n",
            "Final Test Accuracy: 0.8133680820465088\n",
            "Final Test Accuracy: 0.8758680820465088\n",
            "Final Test Accuracy: 0.8888888955116272\n",
            "Final Test Accuracy: 0.890625\n",
            "Final Test Accuracy: 0.874131977558136\n",
            "Final Test Accuracy: 0.7777777910232544\n",
            "Final Test Accuracy: 0.8680555820465088\n",
            "Final Test Accuracy: 0.7708333134651184\n",
            "Final Test Accuracy: 0.8845486044883728\n",
            "Final Test Accuracy: 0.890625\n",
            "Final Test Accuracy: 0.7222222089767456\n",
            "Final Test Accuracy: 0.7777777910232544\n",
            "Final Test Accuracy: 0.3524305522441864\n",
            "Final Test Accuracy: 0.8385416865348816\n",
            "Final Test Accuracy: 0.7786458134651184\n",
            "Final Test Accuracy: 0.7439236044883728\n",
            "Final Test Accuracy: 0.7777777910232544\n",
            "Final Test Accuracy: 0.8133680820465088\n",
            "Final Test Accuracy: 0.7777777910232544\n",
            "Final Test Accuracy: 0.7638888955116272\n",
            "Final Test Accuracy: 0.7777777910232544\n",
            "Final Test Accuracy: 0.796006977558136\n",
            "Final Test Accuracy: 0.7777777910232544\n",
            "Final Test Accuracy: 0.8333333134651184\n",
            "Final Test Accuracy: 0.7777777910232544\n",
            "Final Test Accuracy: 0.8888888955116272\n",
            "Final Test Accuracy: 0.7795138955116272\n",
            "Final Test Accuracy: 0.7777777910232544\n",
            "Final Test Accuracy: 0.7777777910232544\n",
            "Final Test Accuracy: 0.8888888955116272\n",
            "Final Test Accuracy: 0.8333333134651184\n",
            "Final Test Accuracy: 0.8333333134651184\n",
            "Final Test Accuracy: 0.6961805820465088\n",
            "Final Test Accuracy: 0.8888888955116272\n",
            "Final Test Accuracy: 0.7777777910232544\n",
            "Final Test Accuracy: 0.8333333134651184\n",
            "Final Test Accuracy: 0.8315972089767456\n",
            "Final Test Accuracy: 0.741319477558136\n",
            "Final Test Accuracy: 0.8376736044883728\n",
            "Final Test Accuracy: 0.8333333134651184\n",
            "Final Test Accuracy: 0.6111111044883728\n",
            "Final Test Accuracy: 0.8003472089767456\n",
            "Final Test Accuracy: 0.6727430820465088\n",
            "Final Test Accuracy: 0.6111111044883728\n",
            "Final Test Accuracy: 0.8333333134651184\n",
            "Final Test Accuracy: 0.8333333134651184\n",
            "Final Test Accuracy: 0.7222222089767456\n",
            "Final Test Accuracy: 0.7777777910232544\n",
            "Final Test Accuracy: 0.7907986044883728\n",
            "Final Test Accuracy: 0.7777777910232544\n",
            "Final Test Accuracy: 0.7248263955116272\n",
            "Final Test Accuracy: 0.7456597089767456\n",
            "Final Test Accuracy: 0.8133680820465088\n",
            "Final Test Accuracy: 0.8246527910232544\n",
            "Final Test Accuracy: 0.8888888955116272\n",
            "Avg test accuracy:  0.7961545160412788\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mT2FUB2wW8fe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}