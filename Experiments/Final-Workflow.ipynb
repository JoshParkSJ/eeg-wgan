{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cb0ca43",
   "metadata": {},
   "source": [
    "# Final Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1dc450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import os\n",
    "import sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import ShuffleSplit, cross_val_score, train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "\n",
    "from Data import download_EEGBCI\n",
    "\n",
    "if input(\"Are you sure you want to download the entire dataset? (Y/N)\") in \"Yy\":\n",
    "    total_subjects = list(range(1, 110))\n",
    "    runs = list(range(1, 15))\n",
    "    download_EEGBCI(total_subjects, runs, './EEGData', False)\n",
    "    print(\"Downloaded everything!\")\n",
    "\n",
    "else:\n",
    "    print(\"Download cancelled.\")\n",
    "\n",
    "\n",
    "# Only downloading a subset of the dataset\n",
    "subjects = [x for x in range(1,10)]\n",
    "runs = [1, 2]\n",
    "download_EEGBCI(subjects, runs, './EEGData', False)\n",
    "\n",
    "main_folder = './EEGData/MNE-eegbci-data/files/eegmmidb/1.0.0'\n",
    "\n",
    "subdirectories = [f.path for f in os.scandir(main_folder) if f.is_dir()]\n",
    "\n",
    "opened_files = []\n",
    "closed_files = []\n",
    "\n",
    "for subdirectory in subdirectories:\n",
    "    files = os.listdir(subdirectory)\n",
    "    \n",
    "    if len(files) > 0:\n",
    "        for file in files:\n",
    "            \n",
    "            if file[-6:] == '01.edf':\n",
    "                # This is data for eyes opened\n",
    "                eyes_opened = os.path.join(subdirectory, file)\n",
    "                print(eyes_opened)\n",
    "                opened_files.append(eyes_opened)\n",
    "\n",
    "            if file[-6:] == '02.edf':\n",
    "                # This is data for eyes closed\n",
    "                eyes_closed = os.path.join(subdirectory, file)\n",
    "                print(eyes_closed)\n",
    "                closed_files.append(eyes_closed)\n",
    "    else:\n",
    "        print(f\"No files found in {subdirectory}\")\n",
    "        \n",
    "        \n",
    "large_open_data = []\n",
    "# This is a list of all the data for eyes opened\n",
    "large_closed_data = []\n",
    "# This is a list of all the data for eyes closed\n",
    "\n",
    "for data in opened_files:\n",
    "    large_open_data.append(mne.io.read_raw_edf(data, preload=True, verbose=False).get_data(verbose=False))\n",
    "\n",
    "for data in closed_files:\n",
    "    large_closed_data.append(mne.io.read_raw_edf(data, preload=True, verbose=False).get_data(verbose=False))\n",
    "\n",
    "# This is the number of files for eyes opened\n",
    "print(large_open_data.__len__())\n",
    "# This is the number of files for eyes closed\n",
    "print(large_closed_data.__len__())\n",
    "\n",
    "\n",
    "training_data = np.array(large_open_data[0:12])\n",
    "plt.plot(training_data[0, 0, :])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f09b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen 8 \n",
    "\n",
    "\n",
    "NUM_EPOCHS = 200\n",
    "PRINT_INTERVAL = 10 # every 10 epoch, print status\n",
    "BATCH_SIZE = 12\n",
    "NUM_CHANNELS = 64\n",
    "SEQUENCE_LENGTH = 9760\n",
    "LAMBDA_GP = 5\n",
    "\n",
    "training_data = np.array(large_open_data[0:12])\n",
    "training_data = torch.tensor(training_data)\n",
    "\n",
    "\n",
    "def weights_init(model):\n",
    "    for m in model.modules():\n",
    "      if isinstance(m, (nn.Conv1d, nn.ConvTranspose1d)):\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "\n",
    "# Gaussian normal distribution\n",
    "def sample_normal_distribution(i, j, k):\n",
    "    mu = 0\n",
    "    sigma = 0.02\n",
    "    return torch.Tensor(np.random.normal(mu, sigma, (i, j, k)))\n",
    "\n",
    "def gradient_penalty(D, real, fake):\n",
    "    alpha = torch.rand((BATCH_SIZE, 1, 1)).repeat(1, NUM_CHANNELS, SEQUENCE_LENGTH)\n",
    "    interpolated_seq = real * alpha + fake * (1 - alpha)\n",
    "\n",
    "    # Calculate discriminator scores\n",
    "    mixed_scores = D(interpolated_seq)\n",
    "\n",
    "    # Take the gradient of the scores with respect to the images\n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs=interpolated_seq,\n",
    "        outputs=mixed_scores,\n",
    "        grad_outputs=torch.ones_like(mixed_scores),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "    gradient = gradient.view(gradient.shape[0], -1)\n",
    "    gradient_norm = gradient.norm(2, dim=1)\n",
    "    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n",
    "    return gradient_penalty\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ConvTranspose1d(120, 64, kernel_size=9, stride=1, padding=4),\n",
    "            nn.BatchNorm1d(64, affine=True),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.ConvTranspose1d(64, 64, kernel_size=9, stride=1, padding=4),\n",
    "            nn.BatchNorm1d(64, affine=True),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.ConvTranspose1d(64, 64, kernel_size=9, stride=1, padding=4),\n",
    "            nn.BatchNorm1d(64, affine=True),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.ConvTranspose1d(64, 64, kernel_size=9, stride=1, padding=4),\n",
    "            nn.BatchNorm1d(64, affine=True),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.ConvTranspose1d(64, 64, kernel_size=9, stride=1, padding=4),\n",
    "            nn.BatchNorm1d(64, affine=True),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.ConvTranspose1d(64, 64, kernel_size=9, stride=1, padding=4),\n",
    "            nn.BatchNorm1d(64, affine=True),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block(x)\n",
    "        return x\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv1d(64, 64, kernel_size=9, stride=1, padding=4),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.AvgPool1d(kernel_size=2, stride=2),\n",
    "            nn.Conv1d(64, 64, kernel_size=9, stride=1, padding=4),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.AvgPool1d(kernel_size=2, stride=2),\n",
    "            nn.Conv1d(64, 64, kernel_size=9, stride=1, padding=4),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.AvgPool1d(kernel_size=2, stride=2),\n",
    "            nn.Conv1d(64, 64, kernel_size=9, stride=1, padding=4),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.AvgPool1d(kernel_size=2, stride=2),\n",
    "            nn.Conv1d(64, 64, kernel_size=9, stride=1, padding=4),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.AvgPool1d(kernel_size=2, stride=2),\n",
    "            nn.Conv1d(64, 64, kernel_size=9, stride=1, padding=4),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(305, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        x = self.block(x)\n",
    "        return x\n",
    "\n",
    "def train():\n",
    "    critic_err, gen_err, generated_data = [], [], None\n",
    "    G, D = Generator(), Discriminator()\n",
    "    weights_init(G)\n",
    "    weights_init(D)\n",
    "    critic_optimizer = optim.RMSprop(D.parameters(), lr=2e-4)\n",
    "    gen_optimizer = optim.RMSprop(G.parameters(), lr=2e-4)\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        for _ in range(5):\n",
    "            real = training_data.clone().detach()\n",
    "            real.requires_grad=True\n",
    "            noise = sample_normal_distribution(BATCH_SIZE, 120, 305)\n",
    "            fake = G(noise)\n",
    "            critic_real = D(real).reshape(-1) # flatten\n",
    "            critic_fake = D(fake).reshape(-1) # flatten\n",
    "            gp = gradient_penalty(D, real, fake)\n",
    "            critic_loss = -(torch.mean(critic_real) - torch.mean(critic_fake)) + LAMBDA_GP * gp\n",
    "            D.zero_grad()\n",
    "            critic_loss.backward(retain_graph=True)\n",
    "            critic_optimizer.step()\n",
    "\n",
    "        generated_data = fake\n",
    "        gen_fake = D(fake).reshape(-1)\n",
    "        loss_gen = -torch.mean(gen_fake)\n",
    "        G.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        gen_optimizer.step()\n",
    "\n",
    "        critic_err.append(critic_loss.item())\n",
    "        gen_err.append(loss_gen.item())\n",
    "\n",
    "        if epoch % PRINT_INTERVAL == 0:\n",
    "            print(\"Epoch %s: Critic error (%s) Generator err (%s)\" % (epoch, critic_err, gen_err))\n",
    "            values = generated_data[0, 0, :]\n",
    "            plt.plot(values.tolist())\n",
    "            plt.show()\n",
    "            \n",
    "            # plotting G vs D losses\n",
    "            plt.figure(figsize=(10,5))\n",
    "            plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "            plt.plot(gen_err,label=\"Generator\")\n",
    "            plt.plot(critic_err,label=\"Critic\")\n",
    "            plt.xlabel(\"iterations\")\n",
    "            plt.ylabel(\"Loss\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "    return critic_err, gen_err, generated_data, D, G\n",
    "\n",
    "critic_err, gen_err, generated_data1, disc1, gen1 = train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
