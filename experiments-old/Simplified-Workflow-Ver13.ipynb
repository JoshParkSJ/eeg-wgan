{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abb8fe66",
   "metadata": {},
   "source": [
    "# Simplified-Workflow ver13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffed0316",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import os\n",
    "import sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import ShuffleSplit, cross_val_score, train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.fft import rfft, irfft\n",
    "from torch.nn.functional import normalize\n",
    "from scipy.signal import welch\n",
    "from scipy import signal\n",
    "\n",
    "\n",
    "NUM_CHANNELS = 1\n",
    "SEQUENCE_LENGTH = 9760\n",
    "LAMBDA_GP = 5\n",
    "\n",
    "def compute_psd(data, fs, nperseg=256, noverlap=None):\n",
    "    \"\"\"\n",
    "    Compute Power Spectral Density (PSD) using the Welch method.\n",
    "\n",
    "    Parameters:\n",
    "        data (array): EEG data array with shape (n_channels, n_samples).\n",
    "        fs (float): Sampling frequency.\n",
    "        nperseg (int): Length of each segment for PSD estimation.\n",
    "        noverlap (int): Number of overlapping samples between segments.\n",
    "    \n",
    "    Returns:\n",
    "        freqs (array): Frequency values.\n",
    "        psd (array): Power Spectral Density values.\n",
    "    \"\"\"\n",
    "    n_channels, n_samples = data.shape\n",
    "    psd = np.zeros((n_channels, nperseg // 2 + 1))\n",
    "\n",
    "    for ch_idx in range(n_channels):\n",
    "        f, Pxx = plt.psd(data[ch_idx], Fs=fs, NFFT=256, noverlap=128, window=np.hanning(256), scale_by_freq=True)\n",
    "        # Add a small epsilon to avoid zero values\n",
    "        psd[ch_idx] = Pxx + 1e-10\n",
    "\n",
    "    return f, psd\n",
    "\n",
    "def average_across_arrays(generated_data):\n",
    "    return generated_data.mean(dim=0)\n",
    "\n",
    "# Set the sampling frequency\n",
    "fs = 160.0\n",
    "\n",
    "def weights_init(model):\n",
    "    for m in model.modules():\n",
    "      if isinstance(m, (nn.Conv1d, nn.ConvTranspose1d)):\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "\n",
    "# Gaussian normal distribution\n",
    "def sample_normal_distribution(i, j, k):\n",
    "    mu = 0\n",
    "    sigma = 0.02\n",
    "    return torch.Tensor(np.random.normal(mu, sigma, (i, j, k)))\n",
    "\n",
    "def gradient_penalty(D, real, fake):\n",
    "    alpha = torch.rand((BATCH_SIZE, 1, 1)).repeat(1, NUM_CHANNELS, SEQUENCE_LENGTH)\n",
    "    interpolated_seq = real * alpha + fake * (1 - alpha)\n",
    "\n",
    "    # Calculate discriminator scores\n",
    "    mixed_scores = D(interpolated_seq)\n",
    "\n",
    "    # Take the gradient of the scores with respect to the images\n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs=interpolated_seq,\n",
    "        outputs=mixed_scores,\n",
    "        grad_outputs=torch.ones_like(mixed_scores),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "    gradient = gradient.view(gradient.shape[0], -1)\n",
    "    gradient_norm = gradient.norm(2, dim=1)\n",
    "    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n",
    "    return gradient_penalty\n",
    "\n",
    "def plot_everything(generated_data, gen_err, critic_err):\n",
    "    generated_data = generated_data.detach()\n",
    "    \n",
    "    # plotting generated data\n",
    "    values = generated_data[0, 0, :]\n",
    "    plt.plot(values.tolist())\n",
    "    plt.show()\n",
    "\n",
    "    # plotting PSD\n",
    "    averaged_data = average_across_arrays(generated_data)\n",
    "    freqs, psd = compute_psd(averaged_data, fs)\n",
    "    plt.figure(figsize=(10, 6))  # Add this line to create a single figure\n",
    "    for ch_idx in range(NUM_CHANNELS):\n",
    "        plt.semilogy(freqs, psd[ch_idx], label=f'Channel {ch_idx + 1}')\n",
    "    plt.xlabel('Frequency (Hz)')\n",
    "    plt.ylabel('Power/Frequency (dB/Hz)')\n",
    "    plt.show()\n",
    "\n",
    "    # plotting G vs D losses\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "    plt.plot(gen_err,label=\"Generator\")\n",
    "    plt.plot(critic_err,label=\"Critic\")\n",
    "    plt.xlabel(\"iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 83 batch size for eyes_open\n",
    "# 76 batch size for eyes_closed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38df1ec5",
   "metadata": {},
   "source": [
    "# PSD loss with complex model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7720cfe2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 624128000 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 144\u001b[0m\n\u001b[0;32m    140\u001b[0m             plot_everything(fake, critic_err, gen_err)\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m critic_err, gen_err, fake, D, G\n\u001b[1;32m--> 144\u001b[0m critic_err, gen_err, generated, disc, gen \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    146\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(gen, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerator-v13.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    147\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(disc, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiscriminator-v13.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 125\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    123\u001b[0m     D\u001b[38;5;241m.\u001b[39mzero_grad()            \n\u001b[0;32m    124\u001b[0m     loss_critic \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39mmean(critic_real) \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(critic_fake) \u001b[38;5;241m+\u001b[39m (gradient_penalty(D, real, fake) \u001b[38;5;241m*\u001b[39m LAMBDA_GP)\n\u001b[1;32m--> 125\u001b[0m     \u001b[43mloss_critic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m     critic_optimizer\u001b[38;5;241m.\u001b[39mstep()  \n\u001b[0;32m    128\u001b[0m fake \u001b[38;5;241m=\u001b[39m G(sample_normal_distribution(BATCH_SIZE, \u001b[38;5;241m400\u001b[39m, \u001b[38;5;241m305\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 624128000 bytes."
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 300\n",
    "PRINT_INTERVAL = 10\n",
    "BATCH_SIZE = 76 \n",
    "train_loader = DataLoader(\n",
    "    TensorDataset(torch.tensor(np.load(\"training-closed-ch21.npy\")).detach()), \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ConvTranspose1d(400, 400, kernel_size=9, stride=1),\n",
    "            nn.BatchNorm1d(400, affine=True),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.ConvTranspose1d(400, 400, kernel_size=9, stride=1),\n",
    "            nn.BatchNorm1d(400, affine=True),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.ConvTranspose1d(400, 400, kernel_size=9, stride=1),\n",
    "            nn.BatchNorm1d(400, affine=True),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.ConvTranspose1d(400, 400, kernel_size=9, stride=1),\n",
    "            nn.BatchNorm1d(400, affine=True),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.ConvTranspose1d(400, 400, kernel_size=9, stride=1),\n",
    "            nn.BatchNorm1d(400, affine=True),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.ConvTranspose1d(400, 400, kernel_size=9, stride=1),\n",
    "            nn.BatchNorm1d(400, affine=True),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose1d(400, NUM_CHANNELS, kernel_size=9, stride=1),\n",
    "            nn.BatchNorm1d(NUM_CHANNELS, affine=True),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(10272, 9760)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block(x)\n",
    "        return x\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv1d(NUM_CHANNELS, 400, kernel_size=9, stride=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.AvgPool1d(kernel_size=2, stride=2),\n",
    "            nn.Conv1d(400, 400, kernel_size=9, stride=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.AvgPool1d(kernel_size=2, stride=2),\n",
    "            nn.Conv1d(400, 400, kernel_size=9, stride=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.AvgPool1d(kernel_size=2, stride=2),\n",
    "            nn.Conv1d(400, 400, kernel_size=9, stride=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.AvgPool1d(kernel_size=2, stride=2),\n",
    "            nn.Conv1d(400, 400, kernel_size=9, stride=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.AvgPool1d(kernel_size=2, stride=2),\n",
    "            nn.Conv1d(400, 400, kernel_size=9, stride=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.AvgPool1d(kernel_size=2, stride=2),\n",
    "            nn.Conv1d(400, 400, kernel_size=9, stride=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.AvgPool1d(kernel_size=2, stride=2),\n",
    "            nn.Conv1d(400, 400, kernel_size=9, stride=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.AvgPool1d(kernel_size=2, stride=2),\n",
    "            nn.Conv1d(400, NUM_CHANNELS, kernel_size=9, stride=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(22, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        x = self.block(x)\n",
    "        return x\n",
    "\n",
    "def compute_psd_torch(data, fs, nperseg=256, noverlap=128, window='hann'):\n",
    "    n_channels, n_samples = data.shape\n",
    "    psd = np.zeros((n_channels, nperseg // 2 + 1))\n",
    "\n",
    "    for ch_idx in range(n_channels):\n",
    "        f, Pxx = welch(data[ch_idx], fs=fs, window=window, nperseg=nperseg, noverlap=128)\n",
    "        psd[ch_idx] = Pxx + 1e-10\n",
    "    \n",
    "    return f, psd\n",
    "\n",
    "\n",
    "def compute_psd_loss(real, fake):\n",
    "    for btch in range(BATCH_SIZE):\n",
    "        _, psd_real = compute_psd_torch(real[btch], fs)\n",
    "        reals_psd[btch] = torch.from_numpy(psd_real).float()\n",
    "\n",
    "        _, psd_fake = compute_psd_torch(fake[btch], fs)\n",
    "        fakes_psd[btch] = torch.from_numpy(psd_fake).float()\n",
    "\n",
    "    # Normalize each sample independently\n",
    "    return ((F.normalize(reals_psd, p=2, dim=1) - F.normalize(fakes_psd, p=2, dim=1))**2).mean()\n",
    "    \n",
    "def train():\n",
    "    critic_err, gen_err, critic_psd_err, gen_psd_err = [], [], [], []\n",
    "    G, D = Generator(), Discriminator()\n",
    "    weights_init(G)\n",
    "    weights_init(D)\n",
    "    critic_optimizer = optim.RMSprop(D.parameters(), lr=1e-5)\n",
    "    gen_optimizer = optim.RMSprop(G.parameters(), lr=1e-5)\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        (real, ) = next(iter(train_loader))\n",
    "        real.requires_grad = True\n",
    "\n",
    "        for _ in range(5):\n",
    "            fake = G(sample_normal_distribution(BATCH_SIZE, 400, 305))\n",
    "            critic_real = D(real).reshape(-1)\n",
    "            critic_fake = D(fake).reshape(-1)\n",
    "            D.zero_grad()            \n",
    "            loss_critic = -torch.mean(critic_real) + torch.mean(critic_fake) + (gradient_penalty(D, real, fake) * LAMBDA_GP)\n",
    "            loss_critic.backward()\n",
    "            critic_optimizer.step()  \n",
    "\n",
    "        fake = G(sample_normal_distribution(BATCH_SIZE, 400, 305))\n",
    "        gen_fake = D(fake).reshape(-1)\n",
    "        G.zero_grad()\n",
    "        loss_gen = -torch.mean(gen_fake)\n",
    "        loss_gen.backward()\n",
    "        gen_optimizer.step()\n",
    "        \n",
    "        critic_err.append(loss_critic.item())\n",
    "        gen_err.append(loss_gen.item())\n",
    "\n",
    "        if epoch % PRINT_INTERVAL == 0:\n",
    "            print(\"Epoch %s: Critic error (%s) Generator err (%s)\" % (epoch, critic_err, gen_err))\n",
    "            plot_everything(fake, critic_err, gen_err)\n",
    "\n",
    "    return critic_err, gen_err, fake, D, G\n",
    "\n",
    "critic_err, gen_err, generated, disc, gen = train()\n",
    "\n",
    "torch.save(gen, 'generator-v13.pt')\n",
    "torch.save(disc, 'discriminator-v13.pt')\n",
    "np.save('closed-v13.npy', generated.detach())\n",
    "plot_everything(generated, gen_err, critic_err)\n",
    "generated_data_closed = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e8aeba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
